{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVEtBFnLtP4N","executionInfo":{"status":"ok","timestamp":1713876035828,"user_tz":-60,"elapsed":15542,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"5910adca-2982-405e-9ffc-f194c94ad577"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","import numpy as np\n","import scipy.sparse\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import re\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download(\"stopwords\")"],"metadata":{"id":"13gmpalhtmV-","executionInfo":{"status":"ok","timestamp":1713876038882,"user_tz":-60,"elapsed":3068,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5b22852-42ea-49ba-a760-fcbf03420038"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# Config"],"metadata":{"id":"5AMQpbpe0cIA"}},{"cell_type":"code","source":["config_testing = {\n","    \"testdata\": \"test.csv\",            # Input test data file  [MODIFY THIS]\n","    \"model\": \"LR.pickle\",        # Vectorizer pickle file [MODIFY THIS]\n","    \"vectorizer\": \"TFIDF.pickle\",          # Input model file  [MODIFY THIS]\n","}"],"metadata":{"id":"pi6DBak10drn","executionInfo":{"status":"ok","timestamp":1713876038883,"user_tz":-60,"elapsed":6,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"s95qJJ1Oni_6"}},{"cell_type":"code","source":["# Read the CSV file\n","test_data = pd.read_csv(config_testing['testdata'])\n","\n","# Preprocessingn\n","english_stopwords = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_and_lemmatize(word):\n","    if pd.isnull(word):\n","        return [], \"\"\n","    document_words = re.sub('[^a-zA-Z]', ' ', word)  # Remove non-alphabetical characters\n","    document_words = document_words.lower()          # Case-folding\n","    document_words = nltk.word_tokenize(document_words)  # Tokenization\n","    document_words = [lemmatizer.lemmatize(w) for w in document_words if w not in english_stopwords]  # Remove stopwords and lemmatize\n","    return document_words, ' '.join(document_words)  # Return both list of tokens and the combined string\n","\n","\n","# Preprocess and update DataFrame for both 'premise' and 'hypothesis' in training and testing data\n","for df in [test_data]:\n","    df['premise_processed'] = df['premise'].apply(lambda x: preprocess_and_lemmatize(x)[1])\n","    df['hypothesis_processed'] = df['hypothesis'].apply(lambda x: preprocess_and_lemmatize(x)[1])\n","\n","# Convert DataFrame to the format required by your code\n","test_data_formatted = [test_data['premise'].values, test_data['hypothesis'].values]"],"metadata":{"id":"hwdJ5DWIto-r","executionInfo":{"status":"ok","timestamp":1713876041641,"user_tz":-60,"elapsed":2763,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Defining the Model\n"],"metadata":{"id":"x7dgsS3inmJP"}},{"cell_type":"code","source":["def TFIDF_features(data, mode):\n","    list_sentence1 = data[0]\n","    list_sentence2 = data[1]\n","    corpus_sentence1 = [' '.join(str(item).split()) for item in list_sentence1]\n","    corpus_sentence2 = [' '.join(str(item).split()) for item in list_sentence2]\n","\n","    num_samples = len(list_sentence2)\n","\n","    corpus = [corpus_sentence1[ind] + \" \" + corpus_sentence2[ind] for ind in range(num_samples)]\n","\n","    if mode == \"train\":\n","        TFIDF_vect = TfidfVectorizer()\n","        TFIDF_vect.fit(corpus)\n","        with open(config_testing['vectorizer'], \"wb\") as file:\n","            pickle.dump(TFIDF_vect, file)\n","    elif mode == \"test\":\n","        with open(config_testing['vectorizer'], \"rb\") as file:\n","            TFIDF_vect = pickle.load(file)\n","    else:\n","        print(\"Invalid mode selection\")\n","        exit(0)\n","\n","    tfidf_sentecnce1 = TFIDF_vect.transform(corpus_sentence1)\n","    tfidf_sentecnce2 = TFIDF_vect.transform(corpus_sentence2)\n","\n","    tfidf_feature_array = scipy.sparse.hstack((tfidf_sentecnce1, tfidf_sentecnce2))\n","\n","    return tfidf_feature_array\n"],"metadata":{"id":"2UrXkbFNtqok","executionInfo":{"status":"ok","timestamp":1713876041641,"user_tz":-60,"elapsed":5,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation\n"],"metadata":{"id":"p3HqU4ZYv19S"}},{"cell_type":"code","source":["test_data_formatted = [test_data['premise'].values, test_data['hypothesis'].values]"],"metadata":{"id":"BYR0vPT7vzii","executionInfo":{"status":"ok","timestamp":1713876041641,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import (\n","    precision_recall_fscore_support,\n","    confusion_matrix,\n","    classification_report,\n",")\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# logistic_regression_test function\n","def logistic_regression_test(test_data):\n","    # Obtain the TFIDF features\n","    test_feature = TFIDF_features(test_data, \"test\")\n","\n","    # Load the logistic regression model from the pickle file\n","    with open(config_testing['model'], \"rb\") as file:\n","        LR_model = pickle.load(file)\n","\n","    # Test the logistic regression model\n","    pred_labels = LR_model.predict(test_feature)\n","\n","    predictions_df = pd.DataFrame(pred_labels, columns=['prediction'])\n","\n","    # Save predictions to a CSV file\n","    predictions_df.to_csv('./predictions.csv', index=False)\n","    print(\"predictions.csv file has been dumped.\")"],"metadata":{"id":"zZMtOv_0v551","executionInfo":{"status":"ok","timestamp":1713876042044,"user_tz":-60,"elapsed":407,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test data\n","logistic_regression_test(test_data_formatted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85ul67s5wKVI","executionInfo":{"status":"ok","timestamp":1713876042044,"user_tz":-60,"elapsed":5,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"ae77b2eb-540e-443a-cee3-f74a9e87b2b6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["predictions.csv file has been dumped.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2uFCGVQnmHxF","executionInfo":{"status":"ok","timestamp":1713876042045,"user_tz":-60,"elapsed":4,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":8,"outputs":[]}]}