{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14JDhnX2GecXxffrGmuECs-0v0BKcoWzT","timestamp":1712962378913}],"machine_shape":"hm","collapsed_sections":["fqSQRcS_SGlk","pITX8CEukFPQ","1JHga8dlkHRM","j73PAQWUjU5Z","lfHsPcndjmlf","mrmxMo4xj7dO","Eq1VrlQjugP3"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"nob1UCDfX3QF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following code is only required if you intend to store the model and input train/test files on the Google Drive. It mounts the Google Drive onto the running instance.\n","# If you're uploading files during runtime, you can comment out this code.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6UZIWXuBk98","executionInfo":{"status":"ok","timestamp":1713833759193,"user_tz":-60,"elapsed":45985,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"cc7ef6d1-3a37-4064-e10f-2edb28a880a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Config and Hyperparameters"],"metadata":{"id":"6MuuiSqWkDMB"}},{"cell_type":"code","source":["# Configuration parameters for data preprocessing\n","config_preprocessing = {\n","    \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks\",                             # Directory containing input data files\n","    \"embeddings_file\": \"/content/drive/MyDrive/Colab Notebooks/glove.840B.300d.txt\",  # Path to pre-trained word embeddings file from Stanford obtained from  * https://nlp.stanford.edu/projects/glove/\n","    \"target_dir\": \"./SNLI/\",                                                          # Target directory for preprocessed output files\n","    \"lowercase\": False,                                                               # Flag to indicate whether to convert words to lowercase\n","    \"ignore_punctuation\": False,                                                      # Flag to indicate whether to ignore punctuation\n","    \"num_words\": None,                                                                # Number of words to include in vocabulary\n","    \"stopwords\": [],                                                                  # List of stopwords to be ignored during preprocessing.\n","    \"labeldict\": {\"entailment\": 1, \"contradiction\": 0},                               # Mapping of labels to numerical values\n","    \"bos\": \"_BOS_\",                                                                   # Beginning of sentence token\n","    \"eos\": \"_EOS_\"                                                                    # End of sentence token\n","}\n","\n","# Configuration parameters for model training\n","config_training = {\n","    \"train_data\": \"./SNLI/train_data.pkl\",   # Path to preprocessed training data\n","    \"valid_data\": \"./SNLI/dev_data.pkl\",     # Path to preprocessed validation data\n","    \"embeddings\": \"./SNLI/embeddings.pkl\",   # Path to preprocessed word embeddings produced during the preprocessing phase\n","    \"target_dir\": \"./SNLITrain/\",            # Target directory for output files during training\n","    \"hidden_size\": 300,    # Size of hidden layer in the model\n","    \"dropout\": 0.5,        # Dropout probability\n","    \"num_classes\": 2,      # Number of output classes\n","    \"epochs\": 64,          # Number of training epochs\n","    \"batch_size\": 32,      # Batch size for training\n","    \"lr\": 0.0004,          # Learning rate\n","    \"patience\": 5,         # Patience for early stopping\n","    \"max_gradient_norm\": 10.0    # Maximum gradient norm for gradient clipping\n","}"],"metadata":{"id":"xTSywVRia5ST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Defining the Model and Helper Functions"],"metadata":{"id":"fqSQRcS_SGlk"}},{"cell_type":"markdown","source":["## Helper Util Functions Define"],"metadata":{"id":"pITX8CEukFPQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","def sort_by_seq_lens(batch, sequences_lengths, descending=True):\n","    \"\"\"\n","    Sort a batch of padded variable length sequences by their length.\n","\n","    Args:\n","        batch: A batch of padded variable length sequences. The batch should\n","            have the dimensions (batch_size x max_sequence_length x *).\n","        sequences_lengths: A tensor containing the lengths of the sequences in the\n","            input batch. The tensor should be of size (batch_size).\n","        descending: A boolean value indicating whether to sort the sequences\n","            by their lengths in descending order. Defaults to True.\n","\n","    Returns:\n","        sorted_batch: A tensor containing the input batch reordered by\n","            sequences lengths.\n","        sorted_seq_lens: A tensor containing the sorted lengths of the\n","            sequences in the input batch.\n","        sorting_idx: A tensor containing the indices used to permute the input\n","            batch in order to get 'sorted_batch'.\n","        restoration_idx: A tensor containing the indices that can be used to\n","            restore the order of the sequences in 'sorted_batch' so that it\n","            matches the input batch.\n","    \"\"\"\n","    sorted_seq_lens, sorting_index =\\\n","        sequences_lengths.sort(0, descending=descending)\n","\n","    sorted_batch = batch.index_select(0, sorting_index)\n","\n","    idx_range =\\\n","        sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n","    _, reverse_mapping = sorting_index.sort(0, descending=False)\n","    restoration_index = idx_range.index_select(0, reverse_mapping)\n","\n","    return sorted_batch, sorted_seq_lens, sorting_index, restoration_index\n","\n","\n","def get_mask(sequences_batch, sequences_lengths):\n","    \"\"\"\n","    Get the mask for a batch of padded variable length sequences.\n","\n","    Args:\n","        sequences_batch: A batch of padded variable length sequences\n","            containing word indices. Must be a 2-dimensional tensor of size\n","            (batch, sequence).\n","        sequences_lengths: A tensor containing the lengths of the sequences in\n","            'sequences_batch'. Must be of size (batch).\n","\n","    Returns:\n","        A mask of size (batch, max_sequence_length), where max_sequence_length\n","        is the length of the longest sequence in the batch.\n","    \"\"\"\n","    batch_size = sequences_batch.size()[0]\n","    max_length = torch.max(sequences_lengths)\n","    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n","    mask[sequences_batch[:, :max_length] == 0] = 0.0\n","    return mask\n","\n","def masked_softmax(tensor, mask):\n","    \"\"\"\n","    Apply a masked softmax on the last dimension of a tensor.\n","    The input tensor and mask should be of size (batch, *, sequence_length).\n","\n","    Args:\n","        tensor: The tensor on which the softmax function must be applied along\n","            the last dimension.\n","        mask: A mask of the same size as the tensor with 0s in the positions of\n","            the values that must be masked and 1s everywhere else.\n","\n","    Returns:\n","        A tensor of the same size as the inputs containing the result of the\n","        softmax.\n","    \"\"\"\n","    tensor_shape = tensor.size()\n","    reshaped_tensor = tensor.view(-1, tensor_shape[-1])\n","\n","    # Reshape the mask so it matches the size of the input tensor.\n","    while mask.dim() < tensor.dim():\n","        mask = mask.unsqueeze(1)\n","    mask = mask.expand_as(tensor).contiguous().float()\n","    reshaped_mask = mask.view(-1, mask.size()[-1])\n","\n","    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)\n","    result = result * reshaped_mask\n","    # 1e-13 is added to avoid divisions by zero.\n","    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n","\n","    return result.view(*tensor_shape)\n","\n","def weighted_sum(tensor, weights, mask):\n","    \"\"\"\n","    Apply a weighted sum on the vectors along the last dimension of 'tensor',\n","    and mask the vectors in the result with 'mask'.\n","\n","    Args:\n","        tensor: A tensor of vectors on which a weighted sum must be applied.\n","        weights: The weights to use in the weighted sum.\n","        mask: A mask to apply on the result of the weighted sum.\n","\n","    Returns:\n","        A new tensor containing the result of the weighted sum after the mask\n","        has been applied on it.\n","    \"\"\"\n","    weighted_sum = weights.bmm(tensor)\n","\n","    while mask.dim() < weighted_sum.dim():\n","        mask = mask.unsqueeze(1)\n","    mask = mask.transpose(-1, -2)\n","    mask = mask.expand_as(weighted_sum).contiguous().float()\n","\n","    return weighted_sum * mask\n","\n","def replace_masked(tensor, mask, value):\n","    \"\"\"\n","    Replace the all the values of vectors in 'tensor' that are masked in\n","    'masked' by 'value'.\n","\n","    Args:\n","        tensor: The tensor in which the masked vectors must have their values\n","            replaced.\n","        mask: A mask indicating the vectors which must have their values\n","            replaced.\n","        value: The value to place in the masked vectors of 'tensor'.\n","\n","    Returns:\n","        A new tensor of the same size as 'tensor' where the values of the\n","        vectors masked in 'mask' were replaced by 'value'.\n","    \"\"\"\n","    mask = mask.unsqueeze(1).transpose(2, 1)\n","    reverse_mask = 1.0 - mask\n","    values_to_add = value * reverse_mask\n","    return tensor * mask + values_to_add\n","\n","\n","def correct_predictions(output_probabilities, targets):\n","    \"\"\"\n","    Compute the number of predictions that match some target classes in the\n","    output of a model.\n","\n","    Args:\n","        output_probabilities: A tensor of probabilities for different output\n","            classes.\n","        targets: The indices of the actual target classes.\n","\n","    Returns:\n","        The number of correct predictions in 'output_probabilities'.\n","    \"\"\"\n","    _, out_classes = output_probabilities.max(dim=1)\n","    correct = (out_classes == targets).sum()\n","    return correct.item()"],"metadata":{"id":"M31sT2GGiB1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Layers Define"],"metadata":{"id":"1JHga8dlkHRM"}},{"cell_type":"code","source":["class RNNDropout(nn.Dropout):\n","    \"\"\"\n","    Dropout layer for the inputs of RNNs.\n","\n","    Apply the same dropout mask to all the elements of the same sequence in\n","    a batch of sequences of size (batch, sequences_length, embedding_dim).\n","    \"\"\"\n","\n","    def forward(self, sequences_batch):\n","        \"\"\"\n","        Apply dropout to the input batch of sequences.\n","\n","        Args:\n","            sequences_batch: A batch of sequences of vectors that will serve\n","                as input to an RNN.\n","                Tensor of size (batch, sequences_length, emebdding_dim).\n","\n","        Returns:\n","            A new tensor on which dropout has been applied.\n","        \"\"\"\n","        ones = sequences_batch.data.new_ones(sequences_batch.shape[0],\n","                                             sequences_batch.shape[-1])\n","        dropout_mask = nn.functional.dropout(ones, self.p, self.training,\n","                                             inplace=False)\n","        return dropout_mask.unsqueeze(1) * sequences_batch\n","\n","\n","class Seq2SeqEncoder(nn.Module):\n","    \"\"\"\n","    RNN taking variable length padded sequences of vectors as input and\n","    encoding them into padded sequences of vectors of the same length.\n","\n","    This module is useful to handle batches of padded sequences of vectors\n","    that have different lengths and that need to be passed through a RNN.\n","    The sequences are sorted in descending order of their lengths, packed,\n","    passed through the RNN, and the resulting sequences are then padded and\n","    permuted back to the original order of the input sequences.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 rnn_type,\n","                 input_size,\n","                 hidden_size,\n","                 num_layers=1,\n","                 bias=True,\n","                 dropout=0.0,\n","                 bidirectional=False):\n","        \"\"\"\n","        Args:\n","            rnn_type: The type of RNN to use as encoder in the module.\n","                Must be a class inheriting from torch.nn.RNNBase\n","                (such as torch.nn.LSTM for example).\n","            input_size: The number of expected features in the input of the\n","                module.\n","            hidden_size: The number of features in the hidden state of the RNN\n","                used as encoder by the module.\n","            num_layers: The number of recurrent layers in the encoder of the\n","                module. Defaults to 1.\n","            bias: If False, the encoder does not use bias weights b_ih and\n","                b_hh. Defaults to True.\n","            dropout: If non-zero, introduces a dropout layer on the outputs\n","                of each layer of the encoder except the last one, with dropout\n","                probability equal to 'dropout'. Defaults to 0.0.\n","            bidirectional: If True, the encoder of the module is bidirectional.\n","                Defaults to False.\n","        \"\"\"\n","        assert issubclass(rnn_type, nn.RNNBase),\\\n","            \"rnn_type must be a class inheriting from torch.nn.RNNBase\"\n","\n","        super(Seq2SeqEncoder, self).__init__()\n","\n","        self.rnn_type = rnn_type\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bias = bias\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","\n","        self._encoder = rnn_type(input_size,\n","                                 hidden_size,\n","                                 num_layers=num_layers,\n","                                 bias=bias,\n","                                 batch_first=True,\n","                                 dropout=dropout,\n","                                 bidirectional=bidirectional)\n","\n","    def forward(self, sequences_batch, sequences_lengths):\n","        \"\"\"\n","        Args:\n","            sequences_batch: A batch of variable length sequences of vectors.\n","                The batch is assumed to be of size\n","                (batch, sequence, vector_dim).\n","            sequences_lengths: A 1D tensor containing the sizes of the\n","                sequences in the input batch.\n","\n","        Returns:\n","            reordered_outputs: The outputs (hidden states) of the encoder for\n","                the sequences in the input batch, in the same order.\n","        \"\"\"\n","        sorted_batch, sorted_lengths, _, restoration_idx =\\\n","            sort_by_seq_lens(sequences_batch, sequences_lengths)\n","        packed_batch = nn.utils.rnn.pack_padded_sequence(sorted_batch,\n","                                                         sorted_lengths,\n","                                                         batch_first=True)\n","\n","        outputs, _ = self._encoder(packed_batch, None)\n","\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n","                                                      batch_first=True)\n","        reordered_outputs = outputs.index_select(0, restoration_idx)\n","\n","        return reordered_outputs\n","\n","\n","class SoftmaxAttention(nn.Module):\n","    \"\"\"\n","    Attention layer taking premises and hypotheses encoded by an RNN as input\n","    and computing the soft attention between their elements.\n","\n","    The dot product of the encoded vectors in the premises and hypotheses is\n","    first computed. The softmax of the result is then used in a weighted sum\n","    of the vectors of the premises for each element of the hypotheses, and\n","    conversely for the elements of the premises.\n","    \"\"\"\n","\n","    def forward(self,\n","                premise_batch,\n","                premise_mask,\n","                hypothesis_batch,\n","                hypothesis_mask):\n","        \"\"\"\n","        Args:\n","            premise_batch: A batch of sequences of vectors representing the\n","                premises in some NLI task. The batch is assumed to have the\n","                size (batch, sequences, vector_dim).\n","            premise_mask: A mask for the sequences in the premise batch, to\n","                ignore padding data in the sequences during the computation of\n","                the attention.\n","            hypothesis_batch: A batch of sequences of vectors representing the\n","                hypotheses in some NLI task. The batch is assumed to have the\n","                size (batch, sequences, vector_dim).\n","            hypothesis_mask: A mask for the sequences in the hypotheses batch,\n","                to ignore padding data in the sequences during the computation\n","                of the attention.\n","\n","        Returns:\n","            attended_premises: The sequences of attention vectors for the\n","                premises in the input batch.\n","            attended_hypotheses: The sequences of attention vectors for the\n","                hypotheses in the input batch.\n","        \"\"\"\n","        # Dot product between premises and hypotheses in each sequence of\n","        # the batch.\n","        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1)\n","                                                              .contiguous())\n","\n","        # Softmax attention weights.\n","        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\n","        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2)\n","                                                        .contiguous(),\n","                                       premise_mask)\n","\n","        # Weighted sums of the hypotheses for the the premises attention,\n","        # and vice-versa for the attention of the hypotheses.\n","        attended_premises = weighted_sum(hypothesis_batch,\n","                                         prem_hyp_attn,\n","                                         premise_mask)\n","        attended_hypotheses = weighted_sum(premise_batch,\n","                                           hyp_prem_attn,\n","                                           hypothesis_mask)\n","\n","        return attended_premises, attended_hypotheses"],"metadata":{"id":"RPu3S2KDjIt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Define"],"metadata":{"id":"j73PAQWUjU5Z"}},{"cell_type":"code","source":["import string\n","import torch\n","import numpy as np\n","from collections import Counter\n","from torch.utils.data import Dataset\n","import csv\n","\n","class Preprocessor(object):\n","    \"\"\"\n","    Preprocessor class for Natural Language Inference datasets.\n","\n","    The class can be used to read NLI datasets, build worddicts for them\n","    and transform their premises, hypotheses and labels into lists of\n","    integer indices.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 lowercase=False,\n","                 ignore_punctuation=False,\n","                 num_words=None,\n","                 stopwords=[],\n","                 labeldict={},\n","                 bos=None,\n","                 eos=None):\n","        \"\"\"\n","        Args:\n","            lowercase: A boolean indicating whether the words in the datasets\n","                being preprocessed must be lowercased or not. Defaults to\n","                False.\n","            ignore_punctuation: A boolean indicating whether punctuation must\n","                be ignored or not in the datasets preprocessed by the object.\n","            num_words: An integer indicating the number of words to use in the\n","                worddict of the object. If set to None, all the words in the\n","                data are kept. Defaults to None.\n","            stopwords: A list of words that must be ignored when building the\n","                worddict for a dataset. Defaults to an empty list.\n","            bos: A string indicating the symbol to use for the 'beginning of\n","                sentence' token in the data. If set to None, the token isn't\n","                used. Defaults to None.\n","            eos: A string indicating the symbol to use for the 'end of\n","                sentence' token in the data. If set to None, the token isn't\n","                used. Defaults to None.\n","        \"\"\"\n","        self.lowercase = lowercase\n","        self.ignore_punctuation = ignore_punctuation\n","        self.num_words = num_words\n","        self.stopwords = stopwords\n","        self.labeldict = labeldict\n","        self.bos = bos\n","        self.eos = eos\n","\n","    def read_data(self, filepath):\n","        \"\"\"\n","        Read the premises, hypotheses and labels from some NLI dataset's\n","        file and return them in a dictionary. The file should be in the same\n","        form as SNLI's .txt files.\n","\n","        Args:\n","            filepath: The path to a file containing some premises, hypotheses\n","                and labels that must be read. The file should be formatted in\n","                the same way as the SNLI (and MultiNLI) dataset.\n","\n","        Returns:\n","            A dictionary containing three lists, one for the premises, one for\n","            the hypotheses, and one for the labels in the input data.\n","        \"\"\"\n","\n","        with open(filepath, \"r\", encoding=\"utf8\") as input_data:\n","            ids, premises, hypotheses, labels = [], [], [], []\n","\n","            # Translation tables to remove parentheses and punctuation from\n","            # strings.\n","            parentheses_table = str.maketrans({\"(\": None, \")\": None})\n","            punct_table = str.maketrans({key: \" \"\n","                                         for key in string.punctuation})\n","\n","            csv_reader = csv.reader(input_data)\n","            # Skip the header row\n","            next(csv_reader)\n","\n","            # Initialize id counter\n","            id_counter = 1\n","\n","            # Iterate over each row in the CSV file\n","            for row in csv_reader:\n","                # Ignore sentences that have no gold label.\n","                if row[2] not in (0,1, \"0\", \"1\"):\n","                    continue\n","\n","                premise = row[0]\n","                hypothesis = row[1]\n","\n","                if self.lowercase:\n","                    premise = premise.lower()\n","                    hypothesis = hypothesis.lower()\n","\n","                if self.ignore_punctuation:\n","                    premise = premise.translate(punct_table)\n","                    hypothesis = hypothesis.translate(punct_table)\n","\n","\n","                # Each premise and hypothesis is split into a list of words.\n","                premises.append([w for w in premise.rstrip().split()\n","                                 if w not in self.stopwords])\n","                hypotheses.append([w for w in hypothesis.rstrip().split()\n","                                   if w not in self.stopwords])\n","                labels.append(row[2])\n","                ids.append(id_counter)\n","                id_counter += 1\n","\n","            return {\"ids\": ids,\n","                    \"premises\": premises,\n","                    \"hypotheses\": hypotheses,\n","                    \"labels\": labels}\n","\n","    def build_worddict(self, data):\n","        \"\"\"\n","        Build a dictionary associating words to unique integer indices for\n","        some dataset. The worddict can then be used to transform the words\n","        in datasets to their indices.\n","\n","        Args:\n","            data: A dictionary containing the premises, hypotheses and\n","                labels of some NLI dataset, in the format returned by the\n","                'read_data' method of the Preprocessor class.\n","        \"\"\"\n","        words = []\n","        [words.extend(sentence) for sentence in data[\"premises\"]]\n","        [words.extend(sentence) for sentence in data[\"hypotheses\"]]\n","\n","        counts = Counter(words)\n","        num_words = self.num_words\n","\n","        if self.num_words is None:\n","            num_words = len(counts)\n","\n","        self.worddict = {}\n","\n","        # Special indices are used for padding, out-of-vocabulary words, and\n","        # beginning and end of sentence tokens.\n","        self.worddict[\"_PAD_\"] = 0\n","        self.worddict[\"_OOV_\"] = 1\n","\n","        offset = 2\n","        if self.bos:\n","            self.worddict[\"_BOS_\"] = 2\n","            offset += 1\n","        if self.eos:\n","            self.worddict[\"_EOS_\"] = 3\n","            offset += 1\n","\n","\n","        for i, word in enumerate(counts.most_common(num_words)):\n","            self.worddict[word[0]] = i + offset\n","\n","        if self.labeldict == {}:\n","            label_names = set(data[\"labels\"])\n","            self.labeldict = {label_name: i\n","                              for i, label_name in enumerate(label_names)}\n","\n","    def words_to_indices(self, sentence):\n","        \"\"\"\n","        Transform the words in a sentence to their corresponding integer\n","        indices.\n","\n","        Args:\n","            sentence: A list of words that must be transformed to indices.\n","\n","        Returns:\n","            A list of indices.\n","        \"\"\"\n","        indices = []\n","        # Include the beggining of sentence token at the start of the sentence\n","        # if one is defined.\n","        if self.bos:\n","            indices.append(self.worddict[\"_BOS_\"])\n","\n","        for word in sentence:\n","            if word in self.worddict:\n","                index = self.worddict[word]\n","            else:\n","                # Words absent from 'worddict' are treated as a special\n","                # out-of-vocabulary word (OOV).\n","                index = self.worddict[\"_OOV_\"]\n","            indices.append(index)\n","        # Add the end of sentence token at the end of the sentence if one\n","        # is defined.\n","        if self.eos:\n","            indices.append(self.worddict[\"_EOS_\"])\n","\n","        return indices\n","\n","    def indices_to_words(self, indices):\n","        \"\"\"\n","        Transform the indices in a list to their corresponding words in\n","        the object's worddict.\n","\n","        Args:\n","            indices: A list of integer indices corresponding to words in\n","                the Preprocessor's worddict.\n","\n","        Returns:\n","            A list of words.\n","        \"\"\"\n","        return [list(self.worddict.keys())[list(self.worddict.values())\n","                                           .index(i)]\n","                for i in indices]\n","\n","    def transform_to_indices(self, data):\n","        \"\"\"\n","        Transform the words in the premises and hypotheses of a dataset, as\n","        well as their associated labels, to integer indices.\n","\n","        Args:\n","            data: A dictionary containing lists of premises, hypotheses\n","                and labels, in the format returned by the 'read_data'\n","                method of the Preprocessor class.\n","\n","        Returns:\n","            A dictionary containing the transformed premises, hypotheses and\n","            labels.\n","        \"\"\"\n","        transformed_data = {\"ids\": [],\n","                            \"premises\": [],\n","                            \"hypotheses\": [],\n","                            \"labels\": []}\n","\n","        for i, premise in enumerate(data[\"premises\"]):\n","            # Ignore sentences that have a label for which no index was\n","            # defined in 'labeldict'.\n","            label = data[\"labels\"][i]\n","\n","            if label not in self.labeldict.values() and label not in (\"0\", \"1\") :\n","                continue\n","\n","            transformed_data[\"ids\"].append(data[\"ids\"][i])\n","\n","            transformed_data[\"labels\"].append(label)\n","\n","            indices = self.words_to_indices(premise)\n","            transformed_data[\"premises\"].append(indices)\n","\n","            indices = self.words_to_indices(data[\"hypotheses\"][i])\n","            transformed_data[\"hypotheses\"].append(indices)\n","\n","        return transformed_data\n","\n","    def build_embedding_matrix(self, embeddings_file):\n","        \"\"\"\n","        Build an embedding matrix with pretrained weights for object's\n","        worddict.\n","\n","        Args:\n","            embeddings_file: A file containing pretrained word embeddings.\n","\n","        Returns:\n","            A numpy matrix of size (num_words+n_special_tokens, embedding_dim)\n","            containing pretrained word embeddings (the +n_special_tokens is for\n","            the padding and out-of-vocabulary tokens, as well as BOS and EOS if\n","            they're used).\n","        \"\"\"\n","        # Load the word embeddings in a dictionnary.\n","        embeddings = {}\n","        with open(embeddings_file, \"r\", encoding=\"utf8\") as input_data:\n","            for line in input_data:\n","                line = line.split()\n","\n","                try:\n","                    # Check that the second element on the line is the start\n","                    # of the embedding and not another word. Necessary to\n","                    # ignore multiple word lines.\n","                    float(line[1])\n","                    word = line[0]\n","\n","                    if word in self.worddict:\n","                        embeddings[word] = line[1:]\n","\n","                # Ignore lines corresponding to multiple words separated\n","                # by spaces.\n","                except ValueError:\n","                    continue\n","\n","        num_words = len(self.worddict)\n","        embedding_dim = len(list(embeddings.values())[0])\n","        embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","        # Actual building of the embedding matrix.\n","        missed = 0\n","        for word, i in self.worddict.items():\n","            if word in embeddings:\n","                embedding_matrix[i] = np.array(embeddings[word], dtype=float)\n","            else:\n","                if word == \"_PAD_\":\n","                    continue\n","                missed += 1\n","                # Out of vocabulary words are initialised with random gaussian\n","                # samples.\n","                embedding_matrix[i] = np.random.normal(size=(embedding_dim))\n","        print(\"Missed words: \", missed)\n","\n","        return embedding_matrix\n","\n","\n","class NLIDataset(Dataset):\n","    \"\"\"\n","    Dataset class for Natural Language Inference datasets.\n","\n","    The class can be used to read preprocessed datasets where the premises,\n","    hypotheses and labels have been transformed to unique integer indices\n","    (this can be done with the 'preprocess_data' script in the 'scripts'\n","    folder of this repository).\n","    \"\"\"\n","\n","    def __init__(self,\n","                 data,\n","                 padding_idx=0,\n","                 max_premise_length=None,\n","                 max_hypothesis_length=None):\n","        \"\"\"\n","        Args:\n","            data: A dictionary containing the preprocessed premises,\n","                hypotheses and labels of some dataset.\n","            padding_idx: An integer indicating the index being used for the\n","                padding token in the preprocessed data. Defaults to 0.\n","            max_premise_length: An integer indicating the maximum length\n","                accepted for the sequences in the premises. If set to None,\n","                the length of the longest premise in 'data' is used.\n","                Defaults to None.\n","            max_hypothesis_length: An integer indicating the maximum length\n","                accepted for the sequences in the hypotheses. If set to None,\n","                the length of the longest hypothesis in 'data' is used.\n","                Defaults to None.\n","        \"\"\"\n","        self.premises_lengths = [len(seq) for seq in data[\"premises\"]]\n","        self.max_premise_length = max_premise_length\n","\n","        if self.max_premise_length is None:\n","            self.max_premise_length = max(self.premises_lengths)\n","\n","        self.hypotheses_lengths = [len(seq) for seq in data[\"hypotheses\"]]\n","        self.max_hypothesis_length = max_hypothesis_length\n","        if self.max_hypothesis_length is None:\n","            self.max_hypothesis_length = max(self.hypotheses_lengths)\n","\n","        self.num_sequences = len(data[\"premises\"])\n","\n","        self.data = {\"ids\": [],\n","                     \"premises\": torch.ones((self.num_sequences,\n","                                             self.max_premise_length),\n","                                            dtype=torch.long) * padding_idx,\n","                     \"hypotheses\": torch.ones((self.num_sequences,\n","                                               self.max_hypothesis_length),\n","                                              dtype=torch.long) * padding_idx,\n","                     \"labels\": torch.tensor(list(map(int, data[\"labels\"])), dtype=torch.long)}\n","\n","        for i, premise in enumerate(data[\"premises\"]):\n","            self.data[\"ids\"].append(data[\"ids\"][i])\n","            end = min(len(premise), self.max_premise_length)\n","            self.data[\"premises\"][i][:end] = torch.tensor(premise[:end])\n","\n","            hypothesis = data[\"hypotheses\"][i]\n","            end = min(len(hypothesis), self.max_hypothesis_length)\n","            self.data[\"hypotheses\"][i][:end] = torch.tensor(hypothesis[:end])\n","\n","    def __len__(self):\n","        return self.num_sequences\n","\n","    def __getitem__(self, index):\n","        return {\"id\": self.data[\"ids\"][index],\n","                \"premise\": self.data[\"premises\"][index],\n","                \"premise_length\": min(self.premises_lengths[index],\n","                                      self.max_premise_length),\n","                \"hypothesis\": self.data[\"hypotheses\"][index],\n","                \"hypothesis_length\": min(self.hypotheses_lengths[index],\n","                                         self.max_hypothesis_length),\n","                \"label\": self.data[\"labels\"][index]}"],"metadata":{"id":"qTxBX6nhjPNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Define"],"metadata":{"id":"lfHsPcndjmlf"}},{"cell_type":"code","source":["class ESIM(nn.Module):\n","    \"\"\"\n","    Implementation of the ESIM model presented in the paper \"Enhanced LSTM for\n","    Natural Language Inference\" by Chen et al.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 vocab_size,\n","                 embedding_dim,\n","                 hidden_size,\n","                 embeddings=None,\n","                 padding_idx=0,\n","                 dropout=0.5,\n","                 num_classes=3,\n","                 device=\"cpu\"):\n","        \"\"\"\n","        Args:\n","            vocab_size: The size of the vocabulary of embeddings in the model.\n","            embedding_dim: The dimension of the word embeddings.\n","            hidden_size: The size of all the hidden layers in the network.\n","            embeddings: A tensor of size (vocab_size, embedding_dim) containing\n","                pretrained word embeddings. If None, word embeddings are\n","                initialised randomly. Defaults to None.\n","            padding_idx: The index of the padding token in the premises and\n","                hypotheses passed as input to the model. Defaults to 0.\n","            dropout: The dropout rate to use between the layers of the network.\n","                A dropout rate of 0 corresponds to using no dropout at all.\n","                Defaults to 0.5.\n","            num_classes: The number of classes in the output of the network.\n","                Defaults to 3.\n","            device: The name of the device on which the model is being\n","                executed. Defaults to 'cpu'.\n","        \"\"\"\n","        super(ESIM, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_size = hidden_size\n","        self.num_classes = num_classes\n","        self.dropout = dropout\n","        self.device = device\n","\n","        self._word_embedding = nn.Embedding(self.vocab_size,\n","                                            self.embedding_dim,\n","                                            padding_idx=padding_idx,\n","                                            _weight=embeddings)\n","\n","        if self.dropout:\n","            self._rnn_dropout = RNNDropout(p=self.dropout)\n","            # self._rnn_dropout = nn.Dropout(p=self.dropout)\n","\n","        self._encoding = Seq2SeqEncoder(nn.LSTM,\n","                                        self.embedding_dim,\n","                                        self.hidden_size,\n","                                        bidirectional=True)\n","\n","        self._attention = SoftmaxAttention()\n","\n","        self._projection = nn.Sequential(nn.Linear(4*2*self.hidden_size,\n","                                                   self.hidden_size),\n","                                         nn.ReLU())\n","\n","        self._composition = Seq2SeqEncoder(nn.LSTM,\n","                                           self.hidden_size,\n","                                           self.hidden_size,\n","                                           bidirectional=True)\n","\n","        self._classification = nn.Sequential(nn.Dropout(p=self.dropout),\n","                                             nn.Linear(2*4*self.hidden_size,\n","                                                       self.hidden_size),\n","                                             nn.Tanh(),\n","                                             nn.Dropout(p=self.dropout),\n","                                             nn.Linear(self.hidden_size,\n","                                                       self.num_classes))\n","\n","        # Initialize all weights and biases in the model.\n","        self.apply(_init_esim_weights)\n","\n","    def forward(self,\n","                premises,\n","                premises_lengths,\n","                hypotheses,\n","                hypotheses_lengths):\n","        \"\"\"\n","        Args:\n","            premises: A batch of varaible length sequences of word indices\n","                representing premises. The batch is assumed to be of size\n","                (batch, premises_length).\n","            premises_lengths: A 1D tensor containing the lengths of the\n","                premises in 'premises'.\n","            hypothesis: A batch of varaible length sequences of word indices\n","                representing hypotheses. The batch is assumed to be of size\n","                (batch, hypotheses_length).\n","            hypotheses_lengths: A 1D tensor containing the lengths of the\n","                hypotheses in 'hypotheses'.\n","\n","        Returns:\n","            logits: A tensor of size (batch, num_classes) containing the\n","                logits for each output class of the model.\n","            probabilities: A tensor of size (batch, num_classes) containing\n","                the probabilities of each output class in the model.\n","        \"\"\"\n","        premises_mask = get_mask(premises, premises_lengths).to(self.device)\n","        hypotheses_mask = get_mask(hypotheses, hypotheses_lengths)\\\n","            .to(self.device)\n","\n","        embedded_premises = self._word_embedding(premises)\n","        embedded_hypotheses = self._word_embedding(hypotheses)\n","\n","        if self.dropout:\n","            embedded_premises = self._rnn_dropout(embedded_premises)\n","            embedded_hypotheses = self._rnn_dropout(embedded_hypotheses)\n","\n","        encoded_premises = self._encoding(embedded_premises,\n","                                          premises_lengths)\n","        encoded_hypotheses = self._encoding(embedded_hypotheses,\n","                                            hypotheses_lengths)\n","\n","        attended_premises, attended_hypotheses =\\\n","            self._attention(encoded_premises, premises_mask,\n","                            encoded_hypotheses, hypotheses_mask)\n","\n","        enhanced_premises = torch.cat([encoded_premises,\n","                                       attended_premises,\n","                                       encoded_premises - attended_premises,\n","                                       encoded_premises * attended_premises],\n","                                      dim=-1)\n","        enhanced_hypotheses = torch.cat([encoded_hypotheses,\n","                                         attended_hypotheses,\n","                                         encoded_hypotheses -\n","                                         attended_hypotheses,\n","                                         encoded_hypotheses *\n","                                         attended_hypotheses],\n","                                        dim=-1)\n","\n","        projected_premises = self._projection(enhanced_premises)\n","        projected_hypotheses = self._projection(enhanced_hypotheses)\n","\n","        if self.dropout:\n","            projected_premises = self._rnn_dropout(projected_premises)\n","            projected_hypotheses = self._rnn_dropout(projected_hypotheses)\n","\n","        v_ai = self._composition(projected_premises, premises_lengths)\n","        v_bj = self._composition(projected_hypotheses, hypotheses_lengths)\n","\n","        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n","                                                .transpose(2, 1), dim=1)\\\n","            / torch.sum(premises_mask, dim=1, keepdim=True)\n","        v_b_avg = torch.sum(v_bj * hypotheses_mask.unsqueeze(1)\n","                                                  .transpose(2, 1), dim=1)\\\n","            / torch.sum(hypotheses_mask, dim=1, keepdim=True)\n","\n","        v_a_max, _ = replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n","        v_b_max, _ = replace_masked(v_bj, hypotheses_mask, -1e7).max(dim=1)\n","\n","        v = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n","\n","        logits = self._classification(v)\n","        probabilities = nn.functional.softmax(logits, dim=-1)\n","\n","        return logits, probabilities\n","\n","\n","def _init_esim_weights(module):\n","    \"\"\"\n","    Initialise the weights of the ESIM model.\n","    \"\"\"\n","    if isinstance(module, nn.Linear):\n","        nn.init.xavier_uniform_(module.weight.data)\n","        nn.init.constant_(module.bias.data, 0.0)\n","\n","    elif isinstance(module, nn.LSTM):\n","        nn.init.xavier_uniform_(module.weight_ih_l0.data)\n","        nn.init.orthogonal_(module.weight_hh_l0.data)\n","        nn.init.constant_(module.bias_ih_l0.data, 0.0)\n","        nn.init.constant_(module.bias_hh_l0.data, 0.0)\n","        hidden_size = module.bias_hh_l0.data.shape[0] // 4\n","        module.bias_hh_l0.data[hidden_size:(2*hidden_size)] = 1.0\n","\n","        if (module.bidirectional):\n","            nn.init.xavier_uniform_(module.weight_ih_l0_reverse.data)\n","            nn.init.orthogonal_(module.weight_hh_l0_reverse.data)\n","            nn.init.constant_(module.bias_ih_l0_reverse.data, 0.0)\n","            nn.init.constant_(module.bias_hh_l0_reverse.data, 0.0)\n","            module.bias_hh_l0_reverse.data[hidden_size:(2*hidden_size)] = 1.0"],"metadata":{"id":"QBM3MTEJjnn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"mrmxMo4xj7dO"}},{"cell_type":"code","source":["import os\n","import pickle\n","import argparse\n","import fnmatch\n","import json\n","\n","\n","def preprocess_SNLI_data(inputdir,\n","                         embeddings_file,\n","                         targetdir,\n","                         lowercase=False,\n","                         ignore_punctuation=False,\n","                         num_words=None,\n","                         stopwords=[],\n","                         labeldict={},\n","                         bos=None,\n","                         eos=None):\n","    \"\"\"\n","    Preprocess the data from the SNLI corpus so it can be used by the\n","    ESIM model.\n","    Compute a worddict from the train set, and transform the words in\n","    the sentences of the corpus to their indices, as well as the labels.\n","    Build an embedding matrix from pretrained word vectors.\n","    The preprocessed data is saved in pickled form in some target directory.\n","\n","    Args:\n","        inputdir: The path to the directory containing the NLI corpus.\n","        embeddings_file: The path to the file containing the pretrained\n","            word vectors that must be used to build the embedding matrix.\n","        targetdir: The path to the directory where the preprocessed data\n","            must be saved.\n","        lowercase: Boolean value indicating whether to lowercase the premises\n","            and hypotheseses in the input data. Defautls to False.\n","        ignore_punctuation: Boolean value indicating whether to remove\n","            punctuation from the input data. Defaults to False.\n","        num_words: Integer value indicating the size of the vocabulary to use\n","            for the word embeddings. If set to None, all words are kept.\n","            Defaults to None.\n","        stopwords: A list of words that must be ignored when preprocessing\n","            the data. Defaults to an empty list.\n","        bos: A string indicating the symbol to use for beginning of sentence\n","            tokens. If set to None, bos tokens aren't used. Defaults to None.\n","        eos: A string indicating the symbol to use for end of sentence tokens.\n","            If set to None, eos tokens aren't used. Defaults to None.\n","    \"\"\"\n","    if not os.path.exists(targetdir):\n","        os.makedirs(targetdir)\n","\n","    # Retrieve the train, dev data files from the dataset directory.\n","    train_file = \"\"\n","    dev_file = \"\"\n","    for file in os.listdir(inputdir):\n","        if fnmatch.fnmatch(file, \"train.csv\"):\n","            train_file = file\n","        elif fnmatch.fnmatch(file, \"dev.csv\"):\n","            dev_file = file\n","\n","    # -------------------- Train data preprocessing -------------------- #\n","    preprocessor = Preprocessor(lowercase=lowercase,\n","                                ignore_punctuation=ignore_punctuation,\n","                                num_words=num_words,\n","                                stopwords=stopwords,\n","                                labeldict=labeldict,\n","                                bos=bos,\n","                                eos=eos)\n","\n","    print(20*\"=\", \" Preprocessing train set \", 20*\"=\")\n","    print(\"\\t* Reading data...\")\n","    data = preprocessor.read_data(os.path.join(inputdir, train_file))\n","\n","    print(\"\\t* Computing worddict and saving it...\")\n","    preprocessor.build_worddict(data)\n","    with open(os.path.join(targetdir, \"worddict.pkl\"), \"wb\") as pkl_file:\n","        pickle.dump(preprocessor.worddict, pkl_file)\n","\n","    print(\"\\t* Transforming words in premises and hypotheses to indices...\")\n","    transformed_data = preprocessor.transform_to_indices(data)\n","    print(\"\\t* Saving result...\")\n","    with open(os.path.join(targetdir, \"train_data.pkl\"), \"wb\") as pkl_file:\n","        pickle.dump(transformed_data, pkl_file)\n","\n","    # -------------------- Validation data preprocessing -------------------- #\n","    print(20*\"=\", \" Preprocessing dev set \", 20*\"=\")\n","    print(\"\\t* Reading data...\")\n","    data = preprocessor.read_data(os.path.join(inputdir, dev_file))\n","\n","    print(\"\\t* Transforming words in premises and hypotheses to indices...\")\n","    transformed_data = preprocessor.transform_to_indices(data)\n","\n","\n","    print(\"\\t* Saving result...\")\n","    with open(os.path.join(targetdir, \"dev_data.pkl\"), \"wb\") as pkl_file:\n","        pickle.dump(transformed_data, pkl_file)\n","\n","\n","    # -------------------- Embeddings preprocessing -------------------- #\n","    print(20*\"=\", \" Preprocessing embeddings \", 20*\"=\")\n","    print(\"\\t* Building embedding matrix and saving it...\")\n","    embed_matrix = preprocessor.build_embedding_matrix(embeddings_file)\n","    with open(os.path.join(targetdir, \"embeddings.pkl\"), \"wb\") as pkl_file:\n","        pickle.dump(embed_matrix, pkl_file)\n","\n","\n","preprocess_SNLI_data(\n","    config_preprocessing['data_dir'],\n","    config_preprocessing['embeddings_file'],\n","    config_preprocessing['target_dir'],\n","    lowercase=config_preprocessing[\"lowercase\"],\n","    ignore_punctuation=config_preprocessing[\"ignore_punctuation\"],\n","    num_words=config_preprocessing[\"num_words\"],\n","    stopwords=config_preprocessing[\"stopwords\"],\n","    labeldict=config_preprocessing[\"labeldict\"],\n","    bos=config_preprocessing[\"bos\"],\n","    eos=config_preprocessing[\"eos\"]\n",")"],"metadata":{"id":"oGkVJq0-kRrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"6mBJJbaci79v"}},{"cell_type":"code","source":["import time\n","from tqdm import tqdm\n","\n","\n","def train(model,\n","          dataloader,\n","          optimizer,\n","          criterion,\n","          epoch_number,\n","          max_gradient_norm):\n","    \"\"\"\n","    Train a model for one epoch on some input data with a given optimizer and\n","    criterion.\n","\n","    Args:\n","        model: A torch module that must be trained on some input data.\n","        dataloader: A DataLoader object to iterate over the training data.\n","        optimizer: A torch optimizer to use for training on the input model.\n","        criterion: A loss criterion to use for training.\n","        epoch_number: The number of the epoch for which training is performed.\n","        max_gradient_norm: Max. norm for gradient norm clipping.\n","\n","    Returns:\n","        epoch_time: The total time necessary to train the epoch.\n","        epoch_loss: The training loss computed for the epoch.\n","        epoch_accuracy: The accuracy computed for the epoch.\n","    \"\"\"\n","    # Switch the model to train mode.\n","    model.train()\n","    device = model.device\n","\n","    epoch_start = time.time()\n","    batch_time_avg = 0.0\n","    running_loss = 0.0\n","    correct_preds = 0\n","\n","    tqdm_batch_iterator = tqdm(dataloader)\n","    for batch_index, batch in enumerate(tqdm_batch_iterator):\n","        batch_start = time.time()\n","\n","        # Move input and output data to the GPU if it is used.\n","        premises = batch[\"premise\"].to(device)\n","        premises_lengths = batch[\"premise_length\"].to(device)\n","        hypotheses = batch[\"hypothesis\"].to(device)\n","        hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits, probs = model(premises,\n","                              premises_lengths,\n","                              hypotheses,\n","                              hypotheses_lengths)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","\n","        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n","        optimizer.step()\n","\n","        batch_time_avg += time.time() - batch_start\n","        running_loss += loss.item()\n","        correct_preds += correct_predictions(probs, labels)\n","\n","        description = \"Avg. batch proc. time: {:.4f}s, loss: {:.4f}\"\\\n","                      .format(batch_time_avg/(batch_index+1),\n","                              running_loss/(batch_index+1))\n","        tqdm_batch_iterator.set_description(description)\n","\n","    epoch_time = time.time() - epoch_start\n","    epoch_loss = running_loss / len(dataloader)\n","    epoch_accuracy = correct_preds / len(dataloader.dataset)\n","\n","    return epoch_time, epoch_loss, epoch_accuracy\n","\n","\n","def validate(model, dataloader, criterion):\n","    \"\"\"\n","    Compute the loss and accuracy of a model on some validation dataset.\n","\n","    Args:\n","        model: A torch module for which the loss and accuracy must be\n","            computed.\n","        dataloader: A DataLoader object to iterate over the validation data.\n","        criterion: A loss criterion to use for computing the loss.\n","        epoch: The number of the epoch for which validation is performed.\n","        device: The device on which the model is located.\n","\n","    Returns:\n","        epoch_time: The total time to compute the loss and accuracy on the\n","            entire validation set.\n","        epoch_loss: The loss computed on the entire validation set.\n","        epoch_accuracy: The accuracy computed on the entire validation set.\n","    \"\"\"\n","    # Switch to evaluate mode.\n","    model.eval()\n","    device = model.device\n","\n","    epoch_start = time.time()\n","    running_loss = 0.0\n","    running_accuracy = 0.0\n","\n","    # Deactivate autograd for evaluation.\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            # Move input and output data to the GPU if one is used.\n","            premises = batch[\"premise\"].to(device)\n","            premises_lengths = batch[\"premise_length\"].to(device)\n","            hypotheses = batch[\"hypothesis\"].to(device)\n","            hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            logits, probs = model(premises,\n","                                  premises_lengths,\n","                                  hypotheses,\n","                                  hypotheses_lengths)\n","            loss = criterion(logits, labels)\n","\n","            running_loss += loss.item()\n","            running_accuracy += correct_predictions(probs, labels)\n","\n","    epoch_time = time.time() - epoch_start\n","    epoch_loss = running_loss / len(dataloader)\n","    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n","\n","    return epoch_time, epoch_loss, epoch_accuracy"],"metadata":{"id":"twFbwkRTjC1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","\n","def main(train_file,\n","         valid_file,\n","         embeddings_file,\n","         target_dir,\n","         hidden_size=300,\n","         dropout=0.5,\n","         num_classes=2,\n","         epochs=64,\n","         batch_size=32,\n","         lr=0.0004,\n","         patience=5,\n","         max_grad_norm=10.0,\n","         checkpoint=None):\n","    \"\"\"\n","    Train the ESIM model on the SNLI dataset.\n","\n","    Args:\n","        train_file: A path to some preprocessed data that must be used\n","            to train the model.\n","        valid_file: A path to some preprocessed data that must be used\n","            to validate the model.\n","        embeddings_file: A path to some preprocessed word embeddings that\n","            must be used to initialise the model.\n","        target_dir: The path to a directory where the trained model must\n","            be saved.\n","        hidden_size: The size of the hidden layers in the model. Defaults\n","            to 300.\n","        dropout: The dropout rate to use in the model. Defaults to 0.5.\n","        num_classes: The number of classes in the output of the model.\n","            Defaults to 3.\n","        epochs: The maximum number of epochs for training. Defaults to 64.\n","        batch_size: The size of the batches for training. Defaults to 32.\n","        lr: The learning rate for the optimizer. Defaults to 0.0004.\n","        patience: The patience to use for early stopping. Defaults to 5.\n","        checkpoint: A checkpoint from which to continue training. If None,\n","            training starts from scratch. Defaults to None.\n","    \"\"\"\n","    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n","\n","    print(20 * \"=\", \" Preparing for training \", 20 * \"=\")\n","\n","    if not os.path.exists(target_dir):\n","        os.makedirs(target_dir)\n","\n","    # -------------------- Data loading ------------------- #\n","    print(\"\\t* Loading training data...\")\n","    with open(train_file, \"rb\") as pkl:\n","        train_data = NLIDataset(pickle.load(pkl))\n","\n","    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","\n","    print(\"\\t* Loading validation data...\")\n","    with open(valid_file, \"rb\") as pkl:\n","        valid_data = NLIDataset(pickle.load(pkl))\n","\n","    valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n","\n","    # -------------------- Model definition ------------------- #\n","    print(\"\\t* Building model...\")\n","    with open(embeddings_file, \"rb\") as pkl:\n","        embeddings = torch.tensor(pickle.load(pkl), dtype=torch.float)\\\n","                     .to(device)\n","\n","    model = ESIM(embeddings.shape[0],\n","                 embeddings.shape[1],\n","                 hidden_size,\n","                 embeddings=embeddings,\n","                 dropout=dropout,\n","                 num_classes=num_classes,\n","                 device=device).to(device)\n","\n","    # -------------------- Preparation for training  ------------------- #\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                           mode=\"max\",\n","                                                           factor=0.5,\n","                                                           patience=0)\n","\n","    best_score = 0.0\n","    start_epoch = 1\n","\n","    # Data for loss curves plot.\n","    epochs_count = []\n","    train_losses = []\n","    valid_losses = []\n","\n","    # Continuing training from a checkpoint if one was given as argument.\n","    if checkpoint:\n","        checkpoint = torch.load(checkpoint)\n","        start_epoch = checkpoint[\"epoch\"] + 1\n","        best_score = checkpoint[\"best_score\"]\n","\n","        print(\"\\t* Training will continue on existing model from epoch {}...\"\n","              .format(start_epoch))\n","\n","        model.load_state_dict(checkpoint[\"model\"])\n","        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","        epochs_count = checkpoint[\"epochs_count\"]\n","        train_losses = checkpoint[\"train_losses\"]\n","        valid_losses = checkpoint[\"valid_losses\"]\n","\n","    # Compute loss and accuracy before starting (or resuming) training.\n","    _, valid_loss, valid_accuracy = validate(model,\n","                                             valid_loader,\n","                                             criterion)\n","    print(\"\\t* Validation loss before training: {:.4f}, accuracy: {:.4f}%\"\n","          .format(valid_loss, (valid_accuracy*100)))\n","\n","    # -------------------- Training epochs ------------------- #\n","    print(\"\\n\",\n","          20 * \"=\",\n","          \"Training ESIM model on device: {}\".format(device),\n","          20 * \"=\")\n","\n","    patience_counter = 0\n","    for epoch in range(start_epoch, epochs+1):\n","        epochs_count.append(epoch)\n","\n","        print(\"* Training epoch {}:\".format(epoch))\n","        epoch_time, epoch_loss, epoch_accuracy = train(model,\n","                                                       train_loader,\n","                                                       optimizer,\n","                                                       criterion,\n","                                                       epoch,\n","                                                       max_grad_norm)\n","\n","        train_losses.append(epoch_loss)\n","        print(\"-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\"\n","              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n","\n","        print(\"* Validation for epoch {}:\".format(epoch))\n","        epoch_time, epoch_loss, epoch_accuracy = validate(model,\n","                                                          valid_loader,\n","                                                          criterion)\n","\n","        valid_losses.append(epoch_loss)\n","        print(\"-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\\n\"\n","              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n","\n","        # Update the optimizer's learning rate with the scheduler.\n","        scheduler.step(epoch_accuracy)\n","\n","        # Early stopping on validation accuracy.\n","        if epoch_accuracy < best_score:\n","            patience_counter += 1\n","        else:\n","            best_score = epoch_accuracy\n","            patience_counter = 0\n","            # Save the best model. The optimizer is not saved to avoid having\n","            # a checkpoint file that is too heavy to be shared. To resume\n","            # training from the best model, use the 'esim_*.pth.tar'\n","            # checkpoints instead.\n","            torch.save({\"epoch\": epoch,\n","                        \"model\": model.state_dict(),\n","                        \"best_score\": best_score,\n","                        \"epochs_count\": epochs_count,\n","                        \"train_losses\": train_losses,\n","                        \"valid_losses\": valid_losses},\n","                       os.path.join(target_dir, \"best.pth.tar\"))\n","\n","        # Save the model at each epoch.\n","        torch.save({\"epoch\": epoch,\n","                    \"model\": model.state_dict(),\n","                    \"best_score\": best_score,\n","                    \"optimizer\": optimizer.state_dict(),\n","                    \"epochs_count\": epochs_count,\n","                    \"train_losses\": train_losses,\n","                    \"valid_losses\": valid_losses},\n","                   os.path.join(target_dir, \"esim_{}.pth.tar\".format(epoch)))\n","\n","        if patience_counter >= patience:\n","            print(\"-> Early stopping: patience limit reached, stopping...\")\n","            break\n","\n","    # Plotting of the loss curves for the train and validation sets.\n","    plt.figure()\n","    plt.plot(epochs_count, train_losses, \"-r\")\n","    plt.plot(epochs_count, valid_losses, \"-b\")\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.legend([\"Training loss\", \"Validation loss\"])\n","    plt.title(\"Cross entropy loss\")\n","    plt.show()\n","\n","\n","main(config_training[\"train_data\"],\n","      config_training[\"valid_data\"],\n","      config_training[\"embeddings\"],\n","      config_training[\"target_dir\"],\n","      config_training[\"hidden_size\"],\n","      config_training[\"dropout\"],\n","      config_training[\"num_classes\"],\n","      config_training[\"epochs\"],\n","      config_training[\"batch_size\"],\n","      config_training[\"lr\"],\n","      config_training[\"patience\"],\n","      config_training[\"max_gradient_norm\"],\n","\n","      )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2lQnGCozd02r","executionInfo":{"status":"ok","timestamp":1713009904474,"user_tz":-60,"elapsed":6754158,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"955518d5-d680-4b4e-9c41-185e5089e2b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for training  ====================\n","\t* Loading training data...\n","\t* Loading validation data...\n","\t* Building model...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-df67d084ddd8>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n","  sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n"]},{"output_type":"stream","name":"stdout","text":["\t* Validation loss before training: 1.1344, accuracy: 9.5146%\n","\n"," ==================== Training ESIM model on device: cpu ====================\n","* Training epoch 1:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.6900s, loss: 0.6833: 100%|██████████| 842/842 [09:45<00:00,  1.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 585.6847s, loss = 0.6833, accuracy: 58.0612%\n","* Validation for epoch 1:\n","-> Valid. time: 25.5071s, loss: 0.6190, accuracy: 64.7766%\n","\n","* Training epoch 2:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7864s, loss: 0.6195: 100%|██████████| 842/842 [11:07<00:00,  1.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 667.3037s, loss = 0.6195, accuracy: 65.3318%\n","* Validation for epoch 2:\n","-> Valid. time: 25.6266s, loss: 0.5936, accuracy: 66.4539%\n","\n","* Training epoch 3:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.8206s, loss: 0.5727: 100%|██████████| 842/842 [11:36<00:00,  1.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 696.0776s, loss = 0.5727, accuracy: 69.4997%\n","* Validation for epoch 3:\n","-> Valid. time: 31.5739s, loss: 0.5897, accuracy: 67.8937%\n","\n","* Training epoch 4:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7505s, loss: 0.5366: 100%|██████████| 842/842 [10:36<00:00,  1.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 636.5290s, loss = 0.5366, accuracy: 72.7398%\n","* Validation for epoch 4:\n","-> Valid. time: 25.8363s, loss: 0.5712, accuracy: 68.8140%\n","\n","* Training epoch 5:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7506s, loss: 0.4915: 100%|██████████| 842/842 [10:36<00:00,  1.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 636.6322s, loss = 0.4915, accuracy: 75.8351%\n","* Validation for epoch 5:\n","-> Valid. time: 25.5759s, loss: 0.5778, accuracy: 69.5710%\n","\n","* Training epoch 6:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7449s, loss: 0.4485: 100%|██████████| 842/842 [10:31<00:00,  1.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 631.8121s, loss = 0.4485, accuracy: 78.8636%\n","* Validation for epoch 6:\n","-> Valid. time: 25.8175s, loss: 0.6273, accuracy: 68.7250%\n","\n","* Training epoch 7:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7438s, loss: 0.3840: 100%|██████████| 842/842 [10:30<00:00,  1.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 630.9050s, loss = 0.3840, accuracy: 82.4711%\n","* Validation for epoch 7:\n","-> Valid. time: 25.6776s, loss: 0.6268, accuracy: 68.4875%\n","\n","* Training epoch 8:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7446s, loss: 0.3457: 100%|██████████| 842/842 [10:31<00:00,  1.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 631.4955s, loss = 0.3457, accuracy: 84.6422%\n","* Validation for epoch 8:\n","-> Valid. time: 25.9990s, loss: 0.6403, accuracy: 68.9921%\n","\n","* Training epoch 9:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7863s, loss: 0.3256: 100%|██████████| 842/842 [11:07<00:00,  1.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 667.0270s, loss = 0.3256, accuracy: 85.8707%\n","* Validation for epoch 9:\n","-> Valid. time: 26.4282s, loss: 0.6895, accuracy: 68.1164%\n","\n","* Training epoch 10:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.7937s, loss: 0.3181: 100%|██████████| 842/842 [11:13<00:00,  1.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 673.3530s, loss = 0.3181, accuracy: 86.1342%\n","* Validation for epoch 10:\n","-> Valid. time: 26.1765s, loss: 0.6882, accuracy: 68.1609%\n","\n","-> Early stopping: patience limit reached, stopping...\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"Eq1VrlQjugP3"}},{"cell_type":"code","source":["import time\n","import pickle\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n","from torch.utils.data import DataLoader\n","\n","# Function to calculate model accuracy\n","def correct_predictions(probs, labels):\n","    _, preds = torch.max(probs, 1)\n","    return torch.sum(preds == labels).item()\n","\n","# Function to test the model and gather predictions and ground truth\n","def test(model, dataloader):\n","    \"\"\"\n","    Test the accuracy of a model on some labelled test dataset.\n","\n","    Args:\n","        model: The torch module on which testing must be performed.\n","        dataloader: A DataLoader object to iterate over some dataset.\n","\n","    Returns:\n","        batch_time: The average time to predict the classes of a batch.\n","        total_time: The total time to process the whole dataset.\n","        accuracy: The accuracy of the model on the input data.\n","        predictions: The dictionary containing predictions and their corresponding true labels.\n","    \"\"\"\n","    model.eval()  # Switch to eval mode\n","    device = model.device  # Get device from the model\n","\n","    time_start = time.time()\n","    batch_time = 0.0\n","    accuracy = 0.0\n","\n","    predictions = {}\n","    ground_truth = {}\n","\n","    # Deactivate autograd for evaluation\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch_start = time.time()\n","\n","            # Move input and output data to the GPU if used\n","            premises = batch[\"premise\"].to(device)\n","            premises_lengths = batch[\"premise_length\"].to(device)\n","            hypotheses = batch[\"hypothesis\"].to(device)\n","            hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            ids = batch[\"id\"]\n","\n","            _, probs = model(premises, premises_lengths, hypotheses, hypotheses_lengths)\n","\n","            _, preds = torch.max(probs, 1)\n","\n","            accuracy += correct_predictions(probs, labels)\n","            batch_time += time.time() - batch_start\n","\n","            for i, pair_id in enumerate(ids):\n","                predictions[pair_id] = preds[i].item()\n","                ground_truth[pair_id] = labels[i].item()\n","\n","    batch_time /= len(dataloader)\n","    total_time = time.time() - time_start\n","    accuracy /= len(dataloader.dataset)\n","\n","    return batch_time, total_time, accuracy, predictions, ground_truth\n","\n","# Function to evaluate and return confusion matrix\n","def evaluate(predictions, ground_truth):\n","    \"\"\"\n","    Calculate Precision, Recall, F1-score, and Confusion Matrix.\n","\n","    Args:\n","        predictions: Dictionary containing predicted class labels.\n","        ground_truth: Dictionary containing true class labels.\n","\n","    Returns:\n","        precision, recall, f1-score, confusion matrix, and classification report.\n","    \"\"\"\n","    pred_values = list(predictions.values())\n","    true_values = list(ground_truth.values())\n","\n","    # Calculate precision, recall, F1-score, confusion matrix, and classification report\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        true_values, pred_values, average='macro'\n","    )\n","\n","    confusion_mat = confusion_matrix(true_values, pred_values)\n","    class_report = classification_report(true_values, pred_values)\n","\n","    return precision, recall, f1, confusion_mat, class_report\n","\n","# Function to test the model, evaluate, and plot the confusion matrix\n","def main(test_file, pretrained_file, classes, batch_size=32):\n","    \"\"\"\n","    Test the ESIM model with pretrained weights on some dataset.\n","    Plot confusion matrix after evaluation.\n","\n","    Args:\n","        test_file: Path to a file containing preprocessed NLI data.\n","        pretrained_file: Path to a checkpoint with a pretrained model.\n","        classes: List of class names for the confusion matrix.\n","        batch_size: Size of the batches used for testing.\n","    \"\"\"\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    print(20 * \"=\", \" Preparing for testing \", 20 * \"=\")\n","\n","    checkpoint = torch.load(pretrained_file)  # Load the checkpoint\n","\n","    vocab_size = checkpoint[\"model\"][\"_word_embedding.weight\"].size(0)\n","    embedding_dim = checkpoint[\"model\"]['_word_embedding.weight'].size(1)\n","    hidden_size = checkpoint[\"model\"][\"_projection.0.weight\"].size(0)\n","    num_classes = checkpoint[\"model\"][\"_classification.4.weight\"].size(0)\n","\n","    # Load test data\n","    print(\"\\t* Loading test data...\")\n","    with open(test_file, \"rb\") as pkl:\n","        test_data = NLIDataset(pickle.load(pkl))\n","\n","    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","    # Build the model\n","    print(\"\\t* Building model...\")\n","    model = ESIM(vocab_size, embedding_dim, hidden_size, num_classes=num_classes, device=device).to(device)\n","    model.load_state_dict(checkpoint[\"model\"])\n","\n","    print(20 * \"=\", \" Testing ESIM model on device: {} \".format(device), 20 * \"=\")\n","\n","    # Test the model and gather predictions and ground truth\n","    batch_time, total_time, accuracy, predictions, ground_truth = test(model, test_loader)\n","\n","    print(\"\\n\\n-> Average batch processing time: {:.4f}s, total test time: {:.4f}s, accuracy: {:.4f}%\".format(\n","        batch_time, total_time, (accuracy * 100)\n","    ))\n","\n","    # Evaluate and get confusion matrix\n","    precision, recall, f1, confusion_mat, class_report = evaluate(predictions, ground_truth)\n","\n","    print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n","    print(\"\\n\\nConfusion Matrix:\")\n","    print(confusion_mat)\n","    print(\"\\n\\nClassification Report:\")\n","    print(class_report)\n","\n","    # Plot the confusion matrix\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","main(\"dev_data_preprocessed.pkl\", \"/content/drive/MyDrive/Colab Notebooks/best.pth.tar\", [\"contradiction\", \"entailment\"], 32)  # Update test and pretrained file paths\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NqUkVrZVdDso","executionInfo":{"status":"ok","timestamp":1713835794282,"user_tz":-60,"elapsed":29065,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"8439800f-02e6-4cb8-b1e4-7a9e568b298d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for testing  ====================\n","\t* Loading test data...\n","\t* Building model...\n","====================  Testing ESIM model on device: cpu  ====================\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-df67d084ddd8>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n","  sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","-> Average batch processing time: 0.1314s, total test time: 28.1253s, accuracy: 69.5710%\n","\n","Precision: 0.6963, Recall: 0.6963, F1-score: 0.6957\n","\n","\n","Confusion Matrix:\n","[[2332  927]\n"," [1123 2355]]\n","\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.72      0.69      3259\n","           1       0.72      0.68      0.70      3478\n","\n","    accuracy                           0.70      6737\n","   macro avg       0.70      0.70      0.70      6737\n","weighted avg       0.70      0.70      0.70      6737\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# import time\n","# import pickle\n","# import argparse\n","# import torch\n","# import csv\n","\n","# from torch.utils.data import DataLoader\n","\n","# def test(model, dataloader):\n","#     \"\"\"\n","#     Test the accuracy of a model on some labelled test dataset.\n","\n","#     Args:\n","#         model: The torch module on which testing must be performed.\n","#         dataloader: A DataLoader object to iterate over some dataset.\n","\n","#     Returns:\n","#         batch_time: The average time to predict the classes of a batch.\n","#         total_time: The total time to process the whole dataset.\n","#         accuracy: The accuracy of the model on the input data.\n","#     \"\"\"\n","#     # Switch the model to eval mode.\n","#     model.eval()\n","#     device = model.device\n","\n","#     time_start = time.time()\n","#     batch_time = 0.0\n","#     accuracy = 0.0\n","\n","#     predictions = {}\n","\n","#     # Deactivate autograd for evaluation.\n","#     with torch.no_grad():\n","#         for batch in dataloader:\n","#             batch_start = time.time()\n","\n","#             # Move input and output data to the GPU if one is used.\n","#             premises = batch[\"premise\"].to(device)\n","#             premises_lengths = batch[\"premise_length\"].to(device)\n","#             hypotheses = batch[\"hypothesis\"].to(device)\n","#             hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n","#             labels = batch[\"label\"].to(device)\n","\n","#             ids = batch[\"id\"]\n","\n","#             _, probs = model(premises,\n","#                              premises_lengths,\n","#                              hypotheses,\n","#                              hypotheses_lengths)\n","\n","#             _, preds = torch.max(probs, 1)\n","\n","#             accuracy += correct_predictions(probs, labels)\n","#             batch_time += time.time() - batch_start\n","\n","#             for i, pair_id in enumerate(ids):\n","#                 predictions[pair_id] = preds[i].item()\n","\n","\n","#     batch_time /= len(dataloader)\n","#     total_time = time.time() - time_start\n","#     accuracy /= (len(dataloader.dataset))\n","\n","#     with open(\"results.csv\", 'w', newline='') as file:\n","#         writer = csv.writer(file)\n","#         writer.writerow(['Prediction'])  # Writing header\n","#         for pair_id, pred in predictions.items():\n","#             writer.writerow([pred])\n","\n","#     return batch_time, total_time, accuracy\n","\n","\n","# def main(test_file, pretrained_file, batch_size=32):\n","#     \"\"\"\n","#     Test the ESIM model with pretrained weights on some dataset.\n","\n","#     Args:\n","#         test_file: The path to a file containing preprocessed NLI data.\n","#         pretrained_file: The path to a checkpoint produced by the\n","#             'train_model' script.\n","#         vocab_size: The number of words in the vocabulary of the model\n","#             being tested.\n","#         embedding_dim: The size of the embeddings in the model.\n","#         hidden_size: The size of the hidden layers in the model. Must match\n","#             the size used during training. Defaults to 300.\n","#         num_classes: The number of classes in the output of the model. Must\n","#             match the value used during training. Defaults to 3.\n","#         batch_size: The size of the batches used for testing. Defaults to 32.\n","#     \"\"\"\n","#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#     print(20 * \"=\", \" Preparing for testing \", 20 * \"=\")\n","\n","#     checkpoint = torch.load(pretrained_file)\n","\n","#     # Retrieving model parameters from checkpoint.\n","#     vocab_size = checkpoint[\"model\"][\"_word_embedding.weight\"].size(0)\n","#     embedding_dim = checkpoint[\"model\"]['_word_embedding.weight'].size(1)\n","#     hidden_size = checkpoint[\"model\"][\"_projection.0.weight\"].size(0)\n","#     num_classes = checkpoint[\"model\"][\"_classification.4.weight\"].size(0)\n","\n","#     print(\"\\t* Loading test data...\")\n","#     with open(test_file, \"rb\") as pkl:\n","#         test_data = NLIDataset(pickle.load(pkl))\n","\n","#     test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","#     print(\"\\t* Building model...\")\n","#     model = ESIM(vocab_size,\n","#                  embedding_dim,\n","#                  hidden_size,\n","#                  num_classes=num_classes,\n","#                  device=device).to(device)\n","\n","#     model.load_state_dict(checkpoint[\"model\"])\n","\n","#     print(20 * \"=\",\n","#           \" Testing ESIM model on device: {} \".format(device),\n","#           20 * \"=\")\n","#     batch_time, total_time, accuracy = test(model, test_loader)\n","\n","#     print(\"-> Average batch processing time: {:.4f}s, total test time:\\\n","#  {:.4f}s, accuracy: {:.4f}%\".format(batch_time, total_time, (accuracy*100)))\n","\n","# main(\"/content/SNLI/dev_data.pkl\", \"/content/SNLITrain/best.pth.tar\", 32) #checkpoint: Path to a checkpoint with a pretrained model"],"metadata":{"id":"Gt23w2Qn7WWK","executionInfo":{"status":"ok","timestamp":1713833825640,"user_tz":-60,"elapsed":39129,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9c8c05e-904e-499a-de35-2276d0c38875"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for testing  ====================\n","\t* Loading test data...\n","\t* Building model...\n","====================  Testing ESIM model on device: cpu  ====================\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-df67d084ddd8>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n","  sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n"]},{"output_type":"stream","name":"stdout","text":["-> Average batch processing time: 0.1361s, total test time: 28.9428s, accuracy: 69.5710%\n"]}]}]}