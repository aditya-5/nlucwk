{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14JDhnX2GecXxffrGmuECs-0v0BKcoWzT","timestamp":1712962378913}],"machine_shape":"hm","collapsed_sections":["pITX8CEukFPQ","1JHga8dlkHRM","j73PAQWUjU5Z"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow\n","import torch\n","import torch.nn as nn\n","# Additional dependencies are present, but they are being installed in their respective cells."],"metadata":{"id":"nob1UCDfX3QF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713875817192,"user_tz":-60,"elapsed":5798,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"f6bb0a19-96ed-4785-c182-89f7a934eb52"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["# The following code is only required if you intend to store the model and input train/test files on the Google Drive. It mounts the Google Drive onto the running instance.\n","# If you're uploading files during runtime, you can comment out this code.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6UZIWXuBk98","executionInfo":{"status":"ok","timestamp":1713875818974,"user_tz":-60,"elapsed":1804,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"39c6ad54-0973-4f87-b1f5-6da1ff55dd20"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Config (Modify this)"],"metadata":{"id":"6MuuiSqWkDMB"}},{"cell_type":"code","source":["config_preprocessing = {\n","    \"training_input_data_file\": \"/content/drive/MyDrive/Colab Notebooks/train.csv\",            # Input training data file  [MODIFY THIS]\n","    \"test_input_data_file\": \"/content/drive/MyDrive/Colab Notebooks/test.csv\",                  # Input test data file [MODIFY THIS]\n","    \"preprocessed_test_data_file\": \"test_data.pkl\",\n","    \"lowercase\": False,                                                               # Flag to indicate whether to convert words to lowercase\n","    \"ignore_punctuation\": False,                                                      # Flag to indicate whether to ignore punctuation\n","    \"num_words\": None,                                                                # Number of words to include in vocabulary\n","    \"stopwords\": [],                                                                  # List of stopwords to be ignored during preprocessing.\n","    \"labeldict\": {\"entailment\": 1, \"contradiction\": 0},                               # Mapping of labels to numerical values\n","    \"bos\": \"_BOS_\",                                                                   # Beginning of sentence token\n","    \"eos\": \"_EOS_\"                                                                    # End of sentence token\n","}\n","\n","config_testing = {\n","    \"model\": \"/content/drive/MyDrive/Colab Notebooks/best.pth.tar\",                             # Input model file  [MODIFY THIS]\n","    \"predictions_output_file\": \"predictions.csv\",                                               # [OPTIONAL] Predictions/Results file (if a sub-directory, please manually create the directory)\n","}"],"metadata":{"id":"xTSywVRia5ST","executionInfo":{"status":"ok","timestamp":1713875878887,"user_tz":-60,"elapsed":599,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["# Defining the Model and Helper Functions"],"metadata":{"id":"1QqzU3nsS7JW"}},{"cell_type":"markdown","source":["## Helper Util Functions Define"],"metadata":{"id":"pITX8CEukFPQ"}},{"cell_type":"code","source":["\n","\n","def sort_by_seq_lens(batch, sequences_lengths, descending=True):\n","    \"\"\"\n","    Sort a batch of padded variable length sequences by their length.\n","\n","    Args:\n","        batch: A batch of padded variable length sequences. The batch should\n","            have the dimensions (batch_size x max_sequence_length x *).\n","        sequences_lengths: A tensor containing the lengths of the sequences in the\n","            input batch. The tensor should be of size (batch_size).\n","        descending: A boolean value indicating whether to sort the sequences\n","            by their lengths in descending order. Defaults to True.\n","\n","    Returns:\n","        sorted_batch: A tensor containing the input batch reordered by\n","            sequences lengths.\n","        sorted_seq_lens: A tensor containing the sorted lengths of the\n","            sequences in the input batch.\n","        sorting_idx: A tensor containing the indices used to permute the input\n","            batch in order to get 'sorted_batch'.\n","        restoration_idx: A tensor containing the indices that can be used to\n","            restore the order of the sequences in 'sorted_batch' so that it\n","            matches the input batch.\n","    \"\"\"\n","    sorted_seq_lens, sorting_index =\\\n","        sequences_lengths.sort(0, descending=descending)\n","\n","    sorted_batch = batch.index_select(0, sorting_index)\n","\n","    idx_range =\\\n","        sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n","    _, reverse_mapping = sorting_index.sort(0, descending=False)\n","    restoration_index = idx_range.index_select(0, reverse_mapping)\n","\n","    return sorted_batch, sorted_seq_lens, sorting_index, restoration_index\n","\n","\n","def get_mask(sequences_batch, sequences_lengths):\n","    \"\"\"\n","    Get the mask for a batch of padded variable length sequences.\n","\n","    Args:\n","        sequences_batch: A batch of padded variable length sequences\n","            containing word indices. Must be a 2-dimensional tensor of size\n","            (batch, sequence).\n","        sequences_lengths: A tensor containing the lengths of the sequences in\n","            'sequences_batch'. Must be of size (batch).\n","\n","    Returns:\n","        A mask of size (batch, max_sequence_length), where max_sequence_length\n","        is the length of the longest sequence in the batch.\n","    \"\"\"\n","    batch_size = sequences_batch.size()[0]\n","    max_length = torch.max(sequences_lengths)\n","    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n","    mask[sequences_batch[:, :max_length] == 0] = 0.0\n","    return mask\n","\n","def masked_softmax(tensor, mask):\n","    \"\"\"\n","    Apply a masked softmax on the last dimension of a tensor.\n","    The input tensor and mask should be of size (batch, *, sequence_length).\n","\n","    Args:\n","        tensor: The tensor on which the softmax function must be applied along\n","            the last dimension.\n","        mask: A mask of the same size as the tensor with 0s in the positions of\n","            the values that must be masked and 1s everywhere else.\n","\n","    Returns:\n","        A tensor of the same size as the inputs containing the result of the\n","        softmax.\n","    \"\"\"\n","    tensor_shape = tensor.size()\n","    reshaped_tensor = tensor.view(-1, tensor_shape[-1])\n","\n","    # Reshape the mask so it matches the size of the input tensor.\n","    while mask.dim() < tensor.dim():\n","        mask = mask.unsqueeze(1)\n","    mask = mask.expand_as(tensor).contiguous().float()\n","    reshaped_mask = mask.view(-1, mask.size()[-1])\n","\n","    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)\n","    result = result * reshaped_mask\n","    # 1e-13 is added to avoid divisions by zero.\n","    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n","\n","    return result.view(*tensor_shape)\n","\n","def weighted_sum(tensor, weights, mask):\n","    \"\"\"\n","    Apply a weighted sum on the vectors along the last dimension of 'tensor',\n","    and mask the vectors in the result with 'mask'.\n","\n","    Args:\n","        tensor: A tensor of vectors on which a weighted sum must be applied.\n","        weights: The weights to use in the weighted sum.\n","        mask: A mask to apply on the result of the weighted sum.\n","\n","    Returns:\n","        A new tensor containing the result of the weighted sum after the mask\n","        has been applied on it.\n","    \"\"\"\n","    weighted_sum = weights.bmm(tensor)\n","\n","    while mask.dim() < weighted_sum.dim():\n","        mask = mask.unsqueeze(1)\n","    mask = mask.transpose(-1, -2)\n","    mask = mask.expand_as(weighted_sum).contiguous().float()\n","\n","    return weighted_sum * mask\n","\n","def replace_masked(tensor, mask, value):\n","    \"\"\"\n","    Replace the all the values of vectors in 'tensor' that are masked in\n","    'masked' by 'value'.\n","\n","    Args:\n","        tensor: The tensor in which the masked vectors must have their values\n","            replaced.\n","        mask: A mask indicating the vectors which must have their values\n","            replaced.\n","        value: The value to place in the masked vectors of 'tensor'.\n","\n","    Returns:\n","        A new tensor of the same size as 'tensor' where the values of the\n","        vectors masked in 'mask' were replaced by 'value'.\n","    \"\"\"\n","    mask = mask.unsqueeze(1).transpose(2, 1)\n","    reverse_mask = 1.0 - mask\n","    values_to_add = value * reverse_mask\n","    return tensor * mask + values_to_add\n","\n","\n","def correct_predictions(output_probabilities, targets):\n","    \"\"\"\n","    Compute the number of predictions that match some target classes in the\n","    output of a model.\n","\n","    Args:\n","        output_probabilities: A tensor of probabilities for different output\n","            classes.\n","        targets: The indices of the actual target classes.\n","\n","    Returns:\n","        The number of correct predictions in 'output_probabilities'.\n","    \"\"\"\n","    _, out_classes = output_probabilities.max(dim=1)\n","    correct = (out_classes == targets).sum()\n","    return correct.item()"],"metadata":{"id":"M31sT2GGiB1f","executionInfo":{"status":"ok","timestamp":1713875879414,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["## Layers Define"],"metadata":{"id":"1JHga8dlkHRM"}},{"cell_type":"code","source":["class RNNDropout(nn.Dropout):\n","    \"\"\"\n","    Dropout layer for the inputs of RNNs.\n","\n","    Apply the same dropout mask to all the elements of the same sequence in\n","    a batch of sequences of size (batch, sequences_length, embedding_dim).\n","    \"\"\"\n","\n","    def forward(self, sequences_batch):\n","        \"\"\"\n","        Apply dropout to the input batch of sequences.\n","\n","        Args:\n","            sequences_batch: A batch of sequences of vectors that will serve\n","                as input to an RNN.\n","                Tensor of size (batch, sequences_length, emebdding_dim).\n","\n","        Returns:\n","            A new tensor on which dropout has been applied.\n","        \"\"\"\n","        ones = sequences_batch.data.new_ones(sequences_batch.shape[0],\n","                                             sequences_batch.shape[-1])\n","        dropout_mask = nn.functional.dropout(ones, self.p, self.training,\n","                                             inplace=False)\n","        return dropout_mask.unsqueeze(1) * sequences_batch\n","\n","\n","class Seq2SeqEncoder(nn.Module):\n","    \"\"\"\n","    RNN taking variable length padded sequences of vectors as input and\n","    encoding them into padded sequences of vectors of the same length.\n","\n","    This module is useful to handle batches of padded sequences of vectors\n","    that have different lengths and that need to be passed through a RNN.\n","    The sequences are sorted in descending order of their lengths, packed,\n","    passed through the RNN, and the resulting sequences are then padded and\n","    permuted back to the original order of the input sequences.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 rnn_type,\n","                 input_size,\n","                 hidden_size,\n","                 num_layers=1,\n","                 bias=True,\n","                 dropout=0.0,\n","                 bidirectional=False):\n","        \"\"\"\n","        Args:\n","            rnn_type: The type of RNN to use as encoder in the module.\n","                Must be a class inheriting from torch.nn.RNNBase\n","                (such as torch.nn.LSTM for example).\n","            input_size: The number of expected features in the input of the\n","                module.\n","            hidden_size: The number of features in the hidden state of the RNN\n","                used as encoder by the module.\n","            num_layers: The number of recurrent layers in the encoder of the\n","                module. Defaults to 1.\n","            bias: If False, the encoder does not use bias weights b_ih and\n","                b_hh. Defaults to True.\n","            dropout: If non-zero, introduces a dropout layer on the outputs\n","                of each layer of the encoder except the last one, with dropout\n","                probability equal to 'dropout'. Defaults to 0.0.\n","            bidirectional: If True, the encoder of the module is bidirectional.\n","                Defaults to False.\n","        \"\"\"\n","        assert issubclass(rnn_type, nn.RNNBase),\\\n","            \"rnn_type must be a class inheriting from torch.nn.RNNBase\"\n","\n","        super(Seq2SeqEncoder, self).__init__()\n","\n","        self.rnn_type = rnn_type\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bias = bias\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","\n","        self._encoder = rnn_type(input_size,\n","                                 hidden_size,\n","                                 num_layers=num_layers,\n","                                 bias=bias,\n","                                 batch_first=True,\n","                                 dropout=dropout,\n","                                 bidirectional=bidirectional)\n","\n","    def forward(self, sequences_batch, sequences_lengths):\n","        \"\"\"\n","        Args:\n","            sequences_batch: A batch of variable length sequences of vectors.\n","                The batch is assumed to be of size\n","                (batch, sequence, vector_dim).\n","            sequences_lengths: A 1D tensor containing the sizes of the\n","                sequences in the input batch.\n","\n","        Returns:\n","            reordered_outputs: The outputs (hidden states) of the encoder for\n","                the sequences in the input batch, in the same order.\n","        \"\"\"\n","        sorted_batch, sorted_lengths, _, restoration_idx =\\\n","            sort_by_seq_lens(sequences_batch, sequences_lengths)\n","        packed_batch = nn.utils.rnn.pack_padded_sequence(sorted_batch,\n","                                                         sorted_lengths,\n","                                                         batch_first=True)\n","\n","        outputs, _ = self._encoder(packed_batch, None)\n","\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n","                                                      batch_first=True)\n","        reordered_outputs = outputs.index_select(0, restoration_idx)\n","\n","        return reordered_outputs\n","\n","\n","class SoftmaxAttention(nn.Module):\n","    \"\"\"\n","    Attention layer taking premises and hypotheses encoded by an RNN as input\n","    and computing the soft attention between their elements.\n","\n","    The dot product of the encoded vectors in the premises and hypotheses is\n","    first computed. The softmax of the result is then used in a weighted sum\n","    of the vectors of the premises for each element of the hypotheses, and\n","    conversely for the elements of the premises.\n","    \"\"\"\n","\n","    def forward(self,\n","                premise_batch,\n","                premise_mask,\n","                hypothesis_batch,\n","                hypothesis_mask):\n","        \"\"\"\n","        Args:\n","            premise_batch: A batch of sequences of vectors representing the\n","                premises in some NLI task. The batch is assumed to have the\n","                size (batch, sequences, vector_dim).\n","            premise_mask: A mask for the sequences in the premise batch, to\n","                ignore padding data in the sequences during the computation of\n","                the attention.\n","            hypothesis_batch: A batch of sequences of vectors representing the\n","                hypotheses in some NLI task. The batch is assumed to have the\n","                size (batch, sequences, vector_dim).\n","            hypothesis_mask: A mask for the sequences in the hypotheses batch,\n","                to ignore padding data in the sequences during the computation\n","                of the attention.\n","\n","        Returns:\n","            attended_premises: The sequences of attention vectors for the\n","                premises in the input batch.\n","            attended_hypotheses: The sequences of attention vectors for the\n","                hypotheses in the input batch.\n","        \"\"\"\n","        # Dot product between premises and hypotheses in each sequence of\n","        # the batch.\n","        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1)\n","                                                              .contiguous())\n","\n","        # Softmax attention weights.\n","        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\n","        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2)\n","                                                        .contiguous(),\n","                                       premise_mask)\n","\n","        # Weighted sums of the hypotheses for the the premises attention,\n","        # and vice-versa for the attention of the hypotheses.\n","        attended_premises = weighted_sum(hypothesis_batch,\n","                                         prem_hyp_attn,\n","                                         premise_mask)\n","        attended_hypotheses = weighted_sum(premise_batch,\n","                                           hyp_prem_attn,\n","                                           hypothesis_mask)\n","\n","        return attended_premises, attended_hypotheses"],"metadata":{"id":"RPu3S2KDjIt4","executionInfo":{"status":"ok","timestamp":1713875879415,"user_tz":-60,"elapsed":11,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## Data Define"],"metadata":{"id":"j73PAQWUjU5Z"}},{"cell_type":"code","source":["import string\n","import torch\n","import numpy as np\n","from collections import Counter\n","from torch.utils.data import Dataset\n","import csv\n","\n","class Preprocessor(object):\n","    \"\"\"\n","    Preprocessor class for Natural Language Inference datasets.\n","\n","    The class can be used to read NLI datasets, build worddicts for them\n","    and transform their premises and hypotheses into lists of\n","    integer indices.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 lowercase=False,\n","                 ignore_punctuation=False,\n","                 num_words=None,\n","                 stopwords=[],\n","                 labeldict={},\n","                 bos=None,\n","                 eos=None):\n","        \"\"\"\n","        Args:\n","            lowercase: A boolean indicating whether the words in the datasets\n","                being preprocessed must be lowercased or not. Defaults to\n","                False.\n","            ignore_punctuation: A boolean indicating whether punctuation must\n","                be ignored or not in the datasets preprocessed by the object.\n","            num_words: An integer indicating the number of words to use in the\n","                worddict of the object. If set to None, all the words in the\n","                data are kept. Defaults to None.\n","            stopwords: A list of words that must be ignored when building the\n","                worddict for a dataset. Defaults to an empty list.\n","            bos: A string indicating the symbol to use for the 'beginning of\n","                sentence' token in the data. If set to None, the token isn't\n","                used. Defaults to None.\n","            eos: A string indicating the symbol to use for the 'end of\n","                sentence' token in the data. If set to None, the token isn't\n","                used. Defaults to None.\n","        \"\"\"\n","        self.lowercase = lowercase\n","        self.ignore_punctuation = ignore_punctuation\n","        self.num_words = num_words\n","        self.stopwords = stopwords\n","        self.labeldict = labeldict\n","        self.bos = bos\n","        self.eos = eos\n","\n","    def read_data(self, filepath):\n","        \"\"\"\n","        Read the premises, hypotheses and labels from some NLI dataset's\n","        file and return them in a dictionary. The file should be in the same\n","        form as SNLI's .txt files.\n","\n","        Args:\n","            filepath: The path to a file containing some premises, hypotheses\n","                and labels that must be read. The file should be formatted in\n","                the same way as the SNLI (and MultiNLI) dataset.\n","\n","        Returns:\n","            A dictionary containing two lists, one for the premises, one for\n","            the hypotheses\n","        \"\"\"\n","\n","        with open(filepath, \"r\", encoding=\"utf8\") as input_data:\n","            ids, premises, hypotheses = [], [], []\n","\n","            # Translation tables to remove parentheses and punctuation from\n","            # strings.\n","            parentheses_table = str.maketrans({\"(\": None, \")\": None})\n","            punct_table = str.maketrans({key: \" \"\n","                                         for key in string.punctuation})\n","\n","            csv_reader = csv.reader(input_data)\n","            # Skip the header row\n","            next(csv_reader)\n","\n","            # Initialize id counter\n","            id_counter = 1\n","\n","            # Iterate over each row in the CSV file\n","            for row in csv_reader:\n","                premise = row[0]\n","                hypothesis = row[1]\n","\n","                if self.lowercase:\n","                    premise = premise.lower()\n","                    hypothesis = hypothesis.lower()\n","\n","                if self.ignore_punctuation:\n","                    premise = premise.translate(punct_table)\n","                    hypothesis = hypothesis.translate(punct_table)\n","\n","\n","                # Each premise and hypothesis is split into a list of words.\n","                premises.append([w for w in premise.rstrip().split()\n","                                 if w not in self.stopwords])\n","                hypotheses.append([w for w in hypothesis.rstrip().split()\n","                                   if w not in self.stopwords])\n","                ids.append(id_counter)\n","                id_counter += 1\n","\n","            return {\"ids\": ids,\n","                    \"premises\": premises,\n","                    \"hypotheses\": hypotheses}\n","\n","    def build_worddict(self, data):\n","        \"\"\"\n","        Build a dictionary associating words to unique integer indices for\n","        some dataset. The worddict can then be used to transform the words\n","        in datasets to their indices.\n","\n","        Args:\n","            data: A dictionary containing the premises, hypotheses and\n","                labels of some NLI dataset, in the format returned by the\n","                'read_data' method of the Preprocessor class.\n","        \"\"\"\n","        words = []\n","        [words.extend(sentence) for sentence in data[\"premises\"]]\n","        [words.extend(sentence) for sentence in data[\"hypotheses\"]]\n","\n","        counts = Counter(words)\n","        num_words = self.num_words\n","\n","        if self.num_words is None:\n","            num_words = len(counts)\n","\n","        self.worddict = {}\n","\n","        # Special indices are used for padding, out-of-vocabulary words, and\n","        # beginning and end of sentence tokens.\n","        self.worddict[\"_PAD_\"] = 0\n","        self.worddict[\"_OOV_\"] = 1\n","\n","        offset = 2\n","        if self.bos:\n","            self.worddict[\"_BOS_\"] = 2\n","            offset += 1\n","        if self.eos:\n","            self.worddict[\"_EOS_\"] = 3\n","            offset += 1\n","\n","\n","        for i, word in enumerate(counts.most_common(num_words)):\n","            self.worddict[word[0]] = i + offset\n","\n","    def words_to_indices(self, sentence):\n","        \"\"\"\n","        Transform the words in a sentence to their corresponding integer\n","        indices.\n","\n","        Args:\n","            sentence: A list of words that must be transformed to indices.\n","\n","        Returns:\n","            A list of indices.\n","        \"\"\"\n","        indices = []\n","        # Include the beggining of sentence token at the start of the sentence\n","        # if one is defined.\n","        if self.bos:\n","            indices.append(self.worddict[\"_BOS_\"])\n","\n","        for word in sentence:\n","            if word in self.worddict:\n","                index = self.worddict[word]\n","            else:\n","                # Words absent from 'worddict' are treated as a special\n","                # out-of-vocabulary word (OOV).\n","                index = self.worddict[\"_OOV_\"]\n","            indices.append(index)\n","        # Add the end of sentence token at the end of the sentence if one\n","        # is defined.\n","        if self.eos:\n","            indices.append(self.worddict[\"_EOS_\"])\n","\n","        return indices\n","\n","    def indices_to_words(self, indices):\n","        \"\"\"\n","        Transform the indices in a list to their corresponding words in\n","        the object's worddict.\n","\n","        Args:\n","            indices: A list of integer indices corresponding to words in\n","                the Preprocessor's worddict.\n","\n","        Returns:\n","            A list of words.\n","        \"\"\"\n","        return [list(self.worddict.keys())[list(self.worddict.values())\n","                                           .index(i)]\n","                for i in indices]\n","\n","    def transform_to_indices(self, data):\n","        \"\"\"\n","        Transform the words in the premises and hypotheses of a dataset to integer indices.\n","\n","        Args:\n","            data: A dictionary containing lists of premises and hypotheses\n","                , in the format returned by the 'read_data'\n","                method of the Preprocessor class.\n","\n","        Returns:\n","            A dictionary containing the transformed premises and hypotheses\n","        \"\"\"\n","        transformed_data = {\"ids\": [],\n","                            \"premises\": [],\n","                            \"hypotheses\": []}\n","\n","        for i, premise in enumerate(data[\"premises\"]):\n","\n","            transformed_data[\"ids\"].append(data[\"ids\"][i])\n","\n","\n","            indices = self.words_to_indices(premise)\n","            transformed_data[\"premises\"].append(indices)\n","\n","            indices = self.words_to_indices(data[\"hypotheses\"][i])\n","            transformed_data[\"hypotheses\"].append(indices)\n","\n","        return transformed_data\n","\n","    def build_embedding_matrix(self, embeddings_file):\n","        \"\"\"\n","        Build an embedding matrix with pretrained weights for object's\n","        worddict.\n","\n","        Args:\n","            embeddings_file: A file containing pretrained word embeddings.\n","\n","        Returns:\n","            A numpy matrix of size (num_words+n_special_tokens, embedding_dim)\n","            containing pretrained word embeddings (the +n_special_tokens is for\n","            the padding and out-of-vocabulary tokens, as well as BOS and EOS if\n","            they're used).\n","        \"\"\"\n","        # Load the word embeddings in a dictionnary.\n","        embeddings = {}\n","        with open(embeddings_file, \"r\", encoding=\"utf8\") as input_data:\n","            for line in input_data:\n","                line = line.split()\n","\n","                try:\n","                    # Check that the second element on the line is the start\n","                    # of the embedding and not another word. Necessary to\n","                    # ignore multiple word lines.\n","                    float(line[1])\n","                    word = line[0]\n","\n","                    if word in self.worddict:\n","                        embeddings[word] = line[1:]\n","\n","                # Ignore lines corresponding to multiple words separated\n","                # by spaces.\n","                except ValueError:\n","                    continue\n","\n","        num_words = len(self.worddict)\n","        embedding_dim = len(list(embeddings.values())[0])\n","        embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","        # Actual building of the embedding matrix.\n","        missed = 0\n","        for word, i in self.worddict.items():\n","            if word in embeddings:\n","                embedding_matrix[i] = np.array(embeddings[word], dtype=float)\n","            else:\n","                if word == \"_PAD_\":\n","                    continue\n","                missed += 1\n","                # Out of vocabulary words are initialised with random gaussian\n","                # samples.\n","                embedding_matrix[i] = np.random.normal(size=(embedding_dim))\n","        print(\"Missed words: \", missed)\n","\n","        return embedding_matrix\n","\n","\n","class NLIDataset(Dataset):\n","    \"\"\"\n","    Dataset class for Natural Language Inference datasets.\n","\n","    The class can be used to read preprocessed datasets where the premises,\n","    hypotheses and labels have been transformed to unique integer indices\n","    (this can be done with the 'preprocess_data' script in the 'scripts'\n","    folder of this repository).\n","    \"\"\"\n","\n","    def __init__(self,\n","                 data,\n","                 padding_idx=0,\n","                 max_premise_length=None,\n","                 max_hypothesis_length=None):\n","        \"\"\"\n","        Args:\n","            data: A dictionary containing the preprocessed premises,\n","                hypotheses and labels of some dataset.\n","            padding_idx: An integer indicating the index being used for the\n","                padding token in the preprocessed data. Defaults to 0.\n","            max_premise_length: An integer indicating the maximum length\n","                accepted for the sequences in the premises. If set to None,\n","                the length of the longest premise in 'data' is used.\n","                Defaults to None.\n","            max_hypothesis_length: An integer indicating the maximum length\n","                accepted for the sequences in the hypotheses. If set to None,\n","                the length of the longest hypothesis in 'data' is used.\n","                Defaults to None.\n","        \"\"\"\n","        self.premises_lengths = [len(seq) for seq in data[\"premises\"]]\n","        self.max_premise_length = max_premise_length\n","\n","        if self.max_premise_length is None:\n","            self.max_premise_length = max(self.premises_lengths)\n","\n","        self.hypotheses_lengths = [len(seq) for seq in data[\"hypotheses\"]]\n","        self.max_hypothesis_length = max_hypothesis_length\n","        if self.max_hypothesis_length is None:\n","            self.max_hypothesis_length = max(self.hypotheses_lengths)\n","\n","        self.num_sequences = len(data[\"premises\"])\n","\n","        self.data = {\"ids\": [],\n","                     \"premises\": torch.ones((self.num_sequences,\n","                                             self.max_premise_length),\n","                                            dtype=torch.long) * padding_idx,\n","                     \"hypotheses\": torch.ones((self.num_sequences,\n","                                               self.max_hypothesis_length),\n","                                              dtype=torch.long) * padding_idx\n","                    }\n","\n","        for i, premise in enumerate(data[\"premises\"]):\n","            self.data[\"ids\"].append(data[\"ids\"][i])\n","            end = min(len(premise), self.max_premise_length)\n","            self.data[\"premises\"][i][:end] = torch.tensor(premise[:end])\n","\n","            hypothesis = data[\"hypotheses\"][i]\n","            end = min(len(hypothesis), self.max_hypothesis_length)\n","            self.data[\"hypotheses\"][i][:end] = torch.tensor(hypothesis[:end])\n","\n","    def __len__(self):\n","        return self.num_sequences\n","\n","    def __getitem__(self, index):\n","        return {\"id\": self.data[\"ids\"][index],\n","                \"premise\": self.data[\"premises\"][index],\n","                \"premise_length\": min(self.premises_lengths[index],\n","                                      self.max_premise_length),\n","                \"hypothesis\": self.data[\"hypotheses\"][index],\n","                \"hypothesis_length\": min(self.hypotheses_lengths[index],\n","                                         self.max_hypothesis_length)}"],"metadata":{"id":"qTxBX6nhjPNe","executionInfo":{"status":"ok","timestamp":1713875879415,"user_tz":-60,"elapsed":10,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["## Model Define"],"metadata":{"id":"lfHsPcndjmlf"}},{"cell_type":"code","source":["class ESIM(nn.Module):\n","    \"\"\"\n","    Implementation of the ESIM model presented in the paper \"Enhanced LSTM for\n","    Natural Language Inference\" by Chen et al.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 vocab_size,\n","                 embedding_dim,\n","                 hidden_size,\n","                 embeddings=None,\n","                 padding_idx=0,\n","                 dropout=0.5,\n","                 num_classes=3,\n","                 device=\"cpu\"):\n","        \"\"\"\n","        Args:\n","            vocab_size: The size of the vocabulary of embeddings in the model.\n","            embedding_dim: The dimension of the word embeddings.\n","            hidden_size: The size of all the hidden layers in the network.\n","            embeddings: A tensor of size (vocab_size, embedding_dim) containing\n","                pretrained word embeddings. If None, word embeddings are\n","                initialised randomly. Defaults to None.\n","            padding_idx: The index of the padding token in the premises and\n","                hypotheses passed as input to the model. Defaults to 0.\n","            dropout: The dropout rate to use between the layers of the network.\n","                A dropout rate of 0 corresponds to using no dropout at all.\n","                Defaults to 0.5.\n","            num_classes: The number of classes in the output of the network.\n","                Defaults to 3.\n","            device: The name of the device on which the model is being\n","                executed. Defaults to 'cpu'.\n","        \"\"\"\n","        super(ESIM, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_size = hidden_size\n","        self.num_classes = num_classes\n","        self.dropout = dropout\n","        self.device = device\n","\n","        self._word_embedding = nn.Embedding(self.vocab_size,\n","                                            self.embedding_dim,\n","                                            padding_idx=padding_idx,\n","                                            _weight=embeddings)\n","\n","        if self.dropout:\n","            self._rnn_dropout = RNNDropout(p=self.dropout)\n","            # self._rnn_dropout = nn.Dropout(p=self.dropout)\n","\n","        self._encoding = Seq2SeqEncoder(nn.LSTM,\n","                                        self.embedding_dim,\n","                                        self.hidden_size,\n","                                        bidirectional=True)\n","\n","        self._attention = SoftmaxAttention()\n","\n","        self._projection = nn.Sequential(nn.Linear(4*2*self.hidden_size,\n","                                                   self.hidden_size),\n","                                         nn.ReLU())\n","\n","        self._composition = Seq2SeqEncoder(nn.LSTM,\n","                                           self.hidden_size,\n","                                           self.hidden_size,\n","                                           bidirectional=True)\n","\n","        self._classification = nn.Sequential(nn.Dropout(p=self.dropout),\n","                                             nn.Linear(2*4*self.hidden_size,\n","                                                       self.hidden_size),\n","                                             nn.Tanh(),\n","                                             nn.Dropout(p=self.dropout),\n","                                             nn.Linear(self.hidden_size,\n","                                                       self.num_classes))\n","\n","        # Initialize all weights and biases in the model.\n","        self.apply(_init_esim_weights)\n","\n","    def forward(self,\n","                premises,\n","                premises_lengths,\n","                hypotheses,\n","                hypotheses_lengths):\n","        \"\"\"\n","        Args:\n","            premises: A batch of varaible length sequences of word indices\n","                representing premises. The batch is assumed to be of size\n","                (batch, premises_length).\n","            premises_lengths: A 1D tensor containing the lengths of the\n","                premises in 'premises'.\n","            hypothesis: A batch of varaible length sequences of word indices\n","                representing hypotheses. The batch is assumed to be of size\n","                (batch, hypotheses_length).\n","            hypotheses_lengths: A 1D tensor containing the lengths of the\n","                hypotheses in 'hypotheses'.\n","\n","        Returns:\n","            logits: A tensor of size (batch, num_classes) containing the\n","                logits for each output class of the model.\n","            probabilities: A tensor of size (batch, num_classes) containing\n","                the probabilities of each output class in the model.\n","        \"\"\"\n","        premises_mask = get_mask(premises, premises_lengths).to(self.device)\n","        hypotheses_mask = get_mask(hypotheses, hypotheses_lengths)\\\n","            .to(self.device)\n","\n","        embedded_premises = self._word_embedding(premises)\n","        embedded_hypotheses = self._word_embedding(hypotheses)\n","\n","        if self.dropout:\n","            embedded_premises = self._rnn_dropout(embedded_premises)\n","            embedded_hypotheses = self._rnn_dropout(embedded_hypotheses)\n","\n","        encoded_premises = self._encoding(embedded_premises,\n","                                          premises_lengths)\n","        encoded_hypotheses = self._encoding(embedded_hypotheses,\n","                                            hypotheses_lengths)\n","\n","        attended_premises, attended_hypotheses =\\\n","            self._attention(encoded_premises, premises_mask,\n","                            encoded_hypotheses, hypotheses_mask)\n","\n","        enhanced_premises = torch.cat([encoded_premises,\n","                                       attended_premises,\n","                                       encoded_premises - attended_premises,\n","                                       encoded_premises * attended_premises],\n","                                      dim=-1)\n","        enhanced_hypotheses = torch.cat([encoded_hypotheses,\n","                                         attended_hypotheses,\n","                                         encoded_hypotheses -\n","                                         attended_hypotheses,\n","                                         encoded_hypotheses *\n","                                         attended_hypotheses],\n","                                        dim=-1)\n","\n","        projected_premises = self._projection(enhanced_premises)\n","        projected_hypotheses = self._projection(enhanced_hypotheses)\n","\n","        if self.dropout:\n","            projected_premises = self._rnn_dropout(projected_premises)\n","            projected_hypotheses = self._rnn_dropout(projected_hypotheses)\n","\n","        v_ai = self._composition(projected_premises, premises_lengths)\n","        v_bj = self._composition(projected_hypotheses, hypotheses_lengths)\n","\n","        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n","                                                .transpose(2, 1), dim=1)\\\n","            / torch.sum(premises_mask, dim=1, keepdim=True)\n","        v_b_avg = torch.sum(v_bj * hypotheses_mask.unsqueeze(1)\n","                                                  .transpose(2, 1), dim=1)\\\n","            / torch.sum(hypotheses_mask, dim=1, keepdim=True)\n","\n","        v_a_max, _ = replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n","        v_b_max, _ = replace_masked(v_bj, hypotheses_mask, -1e7).max(dim=1)\n","\n","        v = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n","\n","        logits = self._classification(v)\n","        probabilities = nn.functional.softmax(logits, dim=-1)\n","\n","        return logits, probabilities\n","\n","\n","def _init_esim_weights(module):\n","    \"\"\"\n","    Initialise the weights of the ESIM model.\n","    \"\"\"\n","    if isinstance(module, nn.Linear):\n","        nn.init.xavier_uniform_(module.weight.data)\n","        nn.init.constant_(module.bias.data, 0.0)\n","\n","    elif isinstance(module, nn.LSTM):\n","        nn.init.xavier_uniform_(module.weight_ih_l0.data)\n","        nn.init.orthogonal_(module.weight_hh_l0.data)\n","        nn.init.constant_(module.bias_ih_l0.data, 0.0)\n","        nn.init.constant_(module.bias_hh_l0.data, 0.0)\n","        hidden_size = module.bias_hh_l0.data.shape[0] // 4\n","        module.bias_hh_l0.data[hidden_size:(2*hidden_size)] = 1.0\n","\n","        if (module.bidirectional):\n","            nn.init.xavier_uniform_(module.weight_ih_l0_reverse.data)\n","            nn.init.orthogonal_(module.weight_hh_l0_reverse.data)\n","            nn.init.constant_(module.bias_ih_l0_reverse.data, 0.0)\n","            nn.init.constant_(module.bias_hh_l0_reverse.data, 0.0)\n","            module.bias_hh_l0_reverse.data[hidden_size:(2*hidden_size)] = 1.0"],"metadata":{"id":"QBM3MTEJjnn7","executionInfo":{"status":"ok","timestamp":1713875879415,"user_tz":-60,"elapsed":10,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"mrmxMo4xj7dO"}},{"cell_type":"code","source":["import os\n","import pickle\n","import argparse\n","import fnmatch\n","import json\n","\n","\n","def preprocess_SNLI_data(traininginputfile,\n","                         testinputfile,\n","                         preprocessedtestoutputfile,\n","                         lowercase=False,\n","                         ignore_punctuation=False,\n","                         num_words=None,\n","                         stopwords=[],\n","                         labeldict={},\n","                         bos=None,\n","                         eos=None):\n","    \"\"\"\n","    Preprocess the data from the SNLI corpus so it can be used by the\n","    ESIM model.\n","    Compute a worddict from the train set, and transform the words in\n","    the sentences of the corpus to their indices.\n","    Build an embedding matrix from pretrained word vectors.\n","    The preprocessed data is saved in pickled form in some target directory.\n","\n","    Args:\n","        inputdir: The path to the directory containing the NLI corpus.\n","        embeddings_file: The path to the file containing the pretrained\n","            word vectors that must be used to build the embedding matrix.\n","        targetdir: The path to the directory where the preprocessed data\n","            must be saved.\n","        lowercase: Boolean value indicating whether to lowercase the premises\n","            and hypotheseses in the input data. Defautls to False.\n","        ignore_punctuation: Boolean value indicating whether to remove\n","            punctuation from the input data. Defaults to False.\n","        num_words: Integer value indicating the size of the vocabulary to use\n","            for the word embeddings. If set to None, all words are kept.\n","            Defaults to None.\n","        stopwords: A list of words that must be ignored when preprocessing\n","            the data. Defaults to an empty list.\n","        bos: A string indicating the symbol to use for beginning of sentence\n","            tokens. If set to None, bos tokens aren't used. Defaults to None.\n","        eos: A string indicating the symbol to use for end of sentence tokens.\n","            If set to None, eos tokens aren't used. Defaults to None.\n","    \"\"\"\n","    if not os.path.exists(traininginputfile):\n","        print(f\"The training file '{traininginputfile}' does not exist.\")\n","        return\n","\n","\n","    if not os.path.exists(testinputfile):\n","        print(f\"The test file '{testinputfile}' does not exist.\")\n","        return\n","\n","\n","    # -------------------- Computing Word Dict -------------------- #\n","    preprocessor = Preprocessor(lowercase=lowercase,\n","                                ignore_punctuation=ignore_punctuation,\n","                                num_words=num_words,\n","                                stopwords=stopwords,\n","                                labeldict=labeldict,\n","                                bos=bos,\n","                                eos=eos)\n","\n","    print(\"\\t* Reading training data...\")\n","    data = preprocessor.read_data(traininginputfile)\n","    print(\"\\t* Computing worddict...\")\n","    preprocessor.build_worddict(data)\n","\n","    # # -------------------- Test data preprocessing -------------------- #\n","    print(20*\"=\", \" Preprocessing test set \", 20*\"=\")\n","    data = preprocessor.read_data(testinputfile)\n","\n","    print(\"\\t* Transforming words in premises and hypotheses to indices...\")\n","    transformed_data = preprocessor.transform_to_indices(data)\n","    print(\"\\t* Saving result...\")\n","    with open(preprocessedtestoutputfile , \"wb\") as pkl_file:\n","        pickle.dump(transformed_data, pkl_file)\n","\n","preprocess_SNLI_data(\n","    config_preprocessing['training_input_data_file'],\n","    config_preprocessing['test_input_data_file'],\n","    config_preprocessing['preprocessed_test_data_file'],\n","    lowercase=config_preprocessing[\"lowercase\"],\n","    ignore_punctuation=config_preprocessing[\"ignore_punctuation\"],\n","    num_words=config_preprocessing[\"num_words\"],\n","    stopwords=config_preprocessing[\"stopwords\"],\n","    labeldict=config_preprocessing[\"labeldict\"],\n","    bos=config_preprocessing[\"bos\"],\n","    eos=config_preprocessing[\"eos\"]\n",")\n","\n","\n","\n","\n"],"metadata":{"id":"oGkVJq0-kRrP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713875879999,"user_tz":-60,"elapsed":593,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"af4a03a8-b1a9-4cf4-8337-c9277f9af811"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["\t* Reading training data...\n","\t* Computing worddict...\n","====================  Preprocessing test set  ====================\n","\t* Transforming words in premises and hypotheses to indices...\n","\t* Saving result...\n"]}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"Eq1VrlQjugP3"}},{"cell_type":"code","source":["import time\n","import pickle\n","import argparse\n","import torch\n","import csv\n","\n","from torch.utils.data import DataLoader\n","\n","def test(model, dataloader):\n","    \"\"\"\n","    Test the accuracy of a model on some labelled test dataset.\n","\n","    Args:\n","        model: The torch module on which testing must be performed.\n","        dataloader: A DataLoader object to iterate over some dataset.\n","\n","    Returns:\n","        batch_time: The average time to predict the classes of a batch.\n","        total_time: The total time to process the whole dataset.\n","        accuracy: The accuracy of the model on the input data.\n","    \"\"\"\n","    # Switch the model to eval mode.\n","    model.eval()\n","    device = model.device\n","\n","    # Record the starting time for performance evaluation.\n","    time_start = time.time()\n","    batch_time = 0.0\n","    accuracy = 0.0\n","\n","    predictions = {}\n","\n","    # Deactivate autograd for evaluation.\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch_start = time.time()\n","\n","            # Move input and output data to the GPU if one is used.\n","            premises = batch[\"premise\"].to(device)\n","            premises_lengths = batch[\"premise_length\"].to(device)\n","            hypotheses = batch[\"hypothesis\"].to(device)\n","            hypotheses_lengths = batch[\"hypothesis_length\"].to(device)\n","\n","\n","            ids = batch[\"id\"]\n","\n","            # Perform forward pass through the model to obtain predictions.\n","            _, probs = model(premises,\n","                             premises_lengths,\n","                             hypotheses,\n","                             hypotheses_lengths)\n","\n","            # Extract predicted labels from probabilities.\n","            _, preds = torch.max(probs, 1)\n","\n","            batch_time += time.time() - batch_start\n","\n","            # Store predictions in the dictionary along with sample IDs.\n","            for i, pair_id in enumerate(ids):\n","                predictions[pair_id] = preds[i].item()\n","\n","    # Calculate average batch processing time.\n","    batch_time /= len(dataloader)\n","    # Calculate total time taken for evaluation.\n","    total_time = time.time() - time_start\n","\n","\n","    # Write predicted labels to an output file.\n","    with open( config_testing['predictions_output_file'], 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['prediction'])  # Writing header\n","        for pair_id, pred in predictions.items():\n","            writer.writerow([pred])\n","\n","    return batch_time, total_time\n","\n","\n","def main(test_file, pretrained_file, batch_size=32):\n","    \"\"\"\n","    Test the ESIM model with pretrained weights on some dataset.\n","\n","    Args:\n","        test_file: The path to a file containing preprocessed NLI data.\n","        pretrained_file: The path to a checkpoint produced by the\n","            'train_model' script.\n","        vocab_size: The number of words in the vocabulary of the model\n","            being tested.\n","        embedding_dim: The size of the embeddings in the model.\n","        hidden_size: The size of the hidden layers in the model. Must match\n","            the size used during training. Defaults to 300.\n","        num_classes: The number of classes in the output of the model. Must\n","            match the value used during training. Defaults to 3.\n","        batch_size: The size of the batches used for testing. Defaults to 32.\n","    \"\"\"\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    print(20 * \"=\", \" Preparing for testing \", 20 * \"=\")\n","\n","    checkpoint = torch.load(pretrained_file)\n","\n","    # Retrieving model parameters from checkpoint.\n","    vocab_size = checkpoint[\"model\"][\"_word_embedding.weight\"].size(0)\n","    embedding_dim = checkpoint[\"model\"]['_word_embedding.weight'].size(1)\n","    hidden_size = checkpoint[\"model\"][\"_projection.0.weight\"].size(0)\n","    num_classes = checkpoint[\"model\"][\"_classification.4.weight\"].size(0)\n","\n","\n","    print(\"\\t* Loading test data...\")\n","    with open(test_file, \"rb\") as pkl:\n","        test_data = NLIDataset(pickle.load(pkl))\n","\n","    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","    print(\"\\t* Building model...\")\n","    model = ESIM(vocab_size,\n","                 embedding_dim,\n","                 hidden_size,\n","                 num_classes=num_classes,\n","                 device=device).to(device)\n","\n","    model.load_state_dict(checkpoint[\"model\"])\n","\n","    print(20 * \"=\",\n","          \" Testing ESIM model on device: {} \".format(device),\n","          20 * \"=\")\n","    batch_time, total_time = test(model, test_loader)\n","\n","    print(\"-> Average batch processing time: {:.4f}s, total test time:\\\n"," {:.4f}s\".format(batch_time, total_time))\n","\n","main(config_preprocessing['preprocessed_test_data_file'], config_testing['model'], 32) #checkpoint: Path to a checkpoint with a pretrained model"],"metadata":{"id":"Gt23w2Qn7WWK","executionInfo":{"status":"ok","timestamp":1713875888721,"user_tz":-60,"elapsed":8742,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7453cb18-c0a0-488d-a574-09702a18d0e3"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for testing  ====================\n","\t* Loading test data...\n","\t* Building model...\n","====================  Testing ESIM model on device: cpu  ====================\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-44-16f0f9cd2101>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n","  sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n"]},{"output_type":"stream","name":"stdout","text":["-> Average batch processing time: 0.0809s, total test time: 8.4729s\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ef48PuJk5rbE","executionInfo":{"status":"ok","timestamp":1713875888721,"user_tz":-60,"elapsed":16,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}}},"execution_count":49,"outputs":[]}]}